<!DOCTYPE html>
<html lang="ja">

<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Llama 3.1 Swallow</title>
  <meta name='description' content='Large Language Models Enhancing Japanese Language Capability and Maintaining the English Abilities of Llama 3.1 8B and 70B'>

  <link rel="canonical" href="https://swallow-llm.github.io/llama3.1-swallow.en.html">

  <!-- Twitter Card -->
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Llama 3.1 Swallow – Swallow LLM">
  <meta name="twitter:description" content="Large Language Models Enhancing Japanese Language Capability and Maintaining the English Abilities of Llama 3.1 8B and 70B">
  <meta name="twitter:image:src" content="https://swallow-llm.github.io/images/llama3.1-swallow.png">

  <!-- Facebook OpenGraph -->
  <meta property="og:title" content="Llama 3.1 Swallow – Swallow LLM">
  <meta property="og:description" content="Large Language Models Enhancing Japanese Language Capability and Maintaining the English Abilities of Llama 3.1 8B and 70B">
  <meta property="og:image" content="https://swallow-llm.github.io/images/llama3.1-swallow.png">

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;800;900&display=swap" rel="stylesheet">

  <!-- Ionicons -->
  <link href="https://unpkg.com/ionicons@4.5.0/dist/css/ionicons.min.css" rel="stylesheet">

  <!-- Favicon -->
  <link rel="icon" type="image/x-icon" href="https://swallow-llm.github.io/favicon.ico">

  <style>
  
  /*!------------------------------------------------------------------
[MAIN STYLESHEET]
-------------------------------------------------------------------*/.list-reset{list-style-type:none;margin:0;padding:0}.clearfix::after,.clearfix ::before{content:"";display:table;clear:both}.screen-reader-text{clip:rect(1px, 1px, 1px, 1px);height:1px;overflow:hidden;position:absolute !important;width:1px;word-wrap:normal !important}/*! normalize.css v8.0.0 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:rgba(0,0,0,0)}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-0.25em}sup{top:-0.5em}img{border-style:none}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}button,[type=button],[type=reset],[type=submit]{-webkit-appearance:button}button::-moz-focus-inner,[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner{border-style:none;padding:0}button:-moz-focusring,[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details{display:block}summary{display:list-item}template{display:none}[hidden]{display:none}body,h1,h2,h3,h4,h5,h6,p,blockquote,pre,dl,dd,ol,ul,fieldset,legend,figure,hr{margin:0;padding:0}li>ul,li>ol{margin-bottom:0}table{border-collapse:collapse;border-spacing:0}h1,h2,h3,h4,h5,h6,ul,ol,dl,blockquote,p,address,hr,table,fieldset,figure,pre{margin-bottom:30px}ul,ol,dd{margin-left:16px}ul li,ol li{margin-bottom:10px}.highlight{background:#f3f3f3}.highlighter-rouge .highlight{background:#eef}.highlight .c{color:#998;font-style:italic}.highlight .err{color:#a61717;background-color:#e3d2d2}.highlight .k{font-weight:bold}.highlight .o{font-weight:bold}.highlight .cm{color:#998;font-style:italic}.highlight .cp{color:#999;font-weight:bold}.highlight .c1{color:#998;font-style:italic}.highlight .cs{color:#999;font-weight:bold;font-style:italic}.highlight .gd{color:#000;background-color:#fdd}.highlight .gd .x{color:#000;background-color:#faa}.highlight .ge{font-style:italic}.highlight .gr{color:#a00}.highlight .gh{color:#999}.highlight .gi{color:#000;background-color:#dfd}.highlight .gi .x{color:#000;background-color:#afa}.highlight .go{color:#888}.highlight .gp{color:#555}.highlight .gs{font-weight:bold}.highlight .gu{color:#aaa}.highlight .gt{color:#a00}.highlight .kc{font-weight:bold}.highlight .kd{font-weight:bold}.highlight .kp{font-weight:bold}.highlight .kr{font-weight:bold}.highlight .kt{color:#458;font-weight:bold}.highlight .m{color:#099}.highlight .s{color:#d14}.highlight .na{color:teal}.highlight .nb{color:#0086b3}.highlight .nc{color:#458;font-weight:bold}.highlight .no{color:teal}.highlight .ni{color:purple}.highlight .ne{color:#900;font-weight:bold}.highlight .nf{color:#900;font-weight:bold}.highlight .nn{color:#555}.highlight .nt{color:navy}.highlight .nv{color:teal}.highlight .ow{font-weight:bold}.highlight .w{color:#bbb}.highlight .mf{color:#099}.highlight .mh{color:#099}.highlight .mi{color:#099}.highlight .mo{color:#099}.highlight .sb{color:#d14}.highlight .sc{color:#d14}.highlight .sd{color:#d14}.highlight .s2{color:#d14}.highlight .se{color:#d14}.highlight .sh{color:#d14}.highlight .si{color:#d14}.highlight .sx{color:#d14}.highlight .sr{color:#009926}.highlight .s1{color:#d14}.highlight .ss{color:#990073}.highlight .bp{color:#999}.highlight .vc{color:teal}.highlight .vg{color:teal}.highlight .vi{color:teal}.highlight .il{color:#099}.container{max-width:1180px;padding-left:16px;padding-right:16px;margin:0 auto}@media only screen and (max-width: 1300px){.container{max-width:1100px}}@media only screen and (max-width: 1130px){.container{max-width:1000px}}@media only screen and (max-width: 1024px){.container{max-width:760px}}@media only screen and (max-width: 768px){.container{max-width:560px}}@media only screen and (max-width: 576px){.container{max-width:480px}}.row{display:flex;flex-wrap:wrap;flex:0 1 auto;flex-direction:row;box-sizing:border-box;margin-left:-16px;margin-right:-16px}.col{padding-left:16px;padding-right:16px}[class^=col-]{flex:auto}.col-0{width:0%}.col-1{width:8.3333333333%}.col-2{width:16.6666666667%}.col-3{width:25%}.col-4{width:33.3333333333%}.col-5{width:41.6666666667%}.col-6{width:50%}.col-7{width:58.3333333333%}.col-8{width:66.6666666667%}.col-9{width:75%}.col-10{width:83.3333333333%}.col-11{width:91.6666666667%}.col-12{width:100%}.push-0{margin-left:0%}.push-1{margin-left:8.3333333333%}.push-2{margin-left:16.6666666667%}.push-3{margin-left:25%}.push-4{margin-left:33.3333333333%}.push-5{margin-left:41.6666666667%}.push-6{margin-left:50%}.push-7{margin-left:58.3333333333%}.push-8{margin-left:66.6666666667%}.push-9{margin-left:75%}.push-10{margin-left:83.3333333333%}.push-11{margin-left:91.6666666667%}.push-12{margin-left:100%}.pull-0{margin-right:0%}.pull-1{margin-right:8.3333333333%}.pull-2{margin-right:16.6666666667%}.pull-3{margin-right:25%}.pull-4{margin-right:33.3333333333%}.pull-5{margin-right:41.6666666667%}.pull-6{margin-right:50%}.pull-7{margin-right:58.3333333333%}.pull-8{margin-right:66.6666666667%}.pull-9{margin-right:75%}.pull-10{margin-right:83.3333333333%}.pull-11{margin-right:91.6666666667%}.pull-12{margin-right:100%}@media(max-width: 1024px){.col-d-0{width:0%}.col-d-1{width:8.3333333333%}.col-d-2{width:16.6666666667%}.col-d-3{width:25%}.col-d-4{width:33.3333333333%}.col-d-5{width:41.6666666667%}.col-d-6{width:50%}.col-d-7{width:58.3333333333%}.col-d-8{width:66.6666666667%}.col-d-9{width:75%}.col-d-10{width:83.3333333333%}.col-d-11{width:91.6666666667%}.col-d-12{width:100%}.push-d-0{margin-left:0%}.push-d-1{margin-left:8.3333333333%}.push-d-2{margin-left:16.6666666667%}.push-d-3{margin-left:25%}.push-d-4{margin-left:33.3333333333%}.push-d-5{margin-left:41.6666666667%}.push-d-6{margin-left:50%}.push-d-7{margin-left:58.3333333333%}.push-d-8{margin-left:66.6666666667%}.push-d-9{margin-left:75%}.push-d-10{margin-left:83.3333333333%}.push-d-11{margin-left:91.6666666667%}.push-d-12{margin-left:100%}.pull-d-0{margin-right:0%}.pull-d-1{margin-right:8.3333333333%}.pull-d-2{margin-right:16.6666666667%}.pull-d-3{margin-right:25%}.pull-d-4{margin-right:33.3333333333%}.pull-d-5{margin-right:41.6666666667%}.pull-d-6{margin-right:50%}.pull-d-7{margin-right:58.3333333333%}.pull-d-8{margin-right:66.6666666667%}.pull-d-9{margin-right:75%}.pull-d-10{margin-right:83.3333333333%}.pull-d-11{margin-right:91.6666666667%}.pull-d-12{margin-right:100%}}@media(max-width: 768px){.col-t-0{width:0%}.col-t-1{width:8.3333333333%}.col-t-2{width:16.6666666667%}.col-t-3{width:25%}.col-t-4{width:33.3333333333%}.col-t-5{width:41.6666666667%}.col-t-6{width:50%}.col-t-7{width:58.3333333333%}.col-t-8{width:66.6666666667%}.col-t-9{width:75%}.col-t-10{width:83.3333333333%}.col-t-11{width:91.6666666667%}.col-t-12{width:100%}.push-t-0{margin-left:0%}.push-t-1{margin-left:8.3333333333%}.push-t-2{margin-left:16.6666666667%}.push-t-3{margin-left:25%}.push-t-4{margin-left:33.3333333333%}.push-t-5{margin-left:41.6666666667%}.push-t-6{margin-left:50%}.push-t-7{margin-left:58.3333333333%}.push-t-8{margin-left:66.6666666667%}.push-t-9{margin-left:75%}.push-t-10{margin-left:83.3333333333%}.push-t-11{margin-left:91.6666666667%}.push-t-12{margin-left:100%}.pull-t-0{margin-right:0%}.pull-t-1{margin-right:8.3333333333%}.pull-t-2{margin-right:16.6666666667%}.pull-t-3{margin-right:25%}.pull-t-4{margin-right:33.3333333333%}.pull-t-5{margin-right:41.6666666667%}.pull-t-6{margin-right:50%}.pull-t-7{margin-right:58.3333333333%}.pull-t-8{margin-right:66.6666666667%}.pull-t-9{margin-right:75%}.pull-t-10{margin-right:83.3333333333%}.pull-t-11{margin-right:91.6666666667%}.pull-t-12{margin-right:100%}}@media(max-width: 576px){.col-m-0{width:0%}.col-m-1{width:8.3333333333%}.col-m-2{width:16.6666666667%}.col-m-3{width:25%}.col-m-4{width:33.3333333333%}.col-m-5{width:41.6666666667%}.col-m-6{width:50%}.col-m-7{width:58.3333333333%}.col-m-8{width:66.6666666667%}.col-m-9{width:75%}.col-m-10{width:83.3333333333%}.col-m-11{width:91.6666666667%}.col-m-12{width:100%}.push-m-0{margin-left:0%}.push-m-1{margin-left:8.3333333333%}.push-m-2{margin-left:16.6666666667%}.push-m-3{margin-left:25%}.push-m-4{margin-left:33.3333333333%}.push-m-5{margin-left:41.6666666667%}.push-m-6{margin-left:50%}.push-m-7{margin-left:58.3333333333%}.push-m-8{margin-left:66.6666666667%}.push-m-9{margin-left:75%}.push-m-10{margin-left:83.3333333333%}.push-m-11{margin-left:91.6666666667%}.push-m-12{margin-left:100%}.pull-m-0{margin-right:0%}.pull-m-1{margin-right:8.3333333333%}.pull-m-2{margin-right:16.6666666667%}.pull-m-3{margin-right:25%}.pull-m-4{margin-right:33.3333333333%}.pull-m-5{margin-right:41.6666666667%}.pull-m-6{margin-right:50%}.pull-m-7{margin-right:58.3333333333%}.pull-m-8{margin-right:66.6666666667%}.pull-m-9{margin-right:75%}.pull-m-10{margin-right:83.3333333333%}.pull-m-11{margin-right:91.6666666667%}.pull-m-12{margin-right:100%}}@media(max-width: 1024px){.col-d-0{width:0%}.col-d-1{width:8.3333333333%}.col-d-2{width:16.6666666667%}.col-d-3{width:25%}.col-d-4{width:33.3333333333%}.col-d-5{width:41.6666666667%}.col-d-6{width:50%}.col-d-7{width:58.3333333333%}.col-d-8{width:66.6666666667%}.col-d-9{width:75%}.col-d-10{width:83.3333333333%}.col-d-11{width:91.6666666667%}.col-d-12{width:100%}.push-d-0{margin-left:0%}.push-d-1{margin-left:8.3333333333%}.push-d-2{margin-left:16.6666666667%}.push-d-3{margin-left:25%}.push-d-4{margin-left:33.3333333333%}.push-d-5{margin-left:41.6666666667%}.push-d-6{margin-left:50%}.push-d-7{margin-left:58.3333333333%}.push-d-8{margin-left:66.6666666667%}.push-d-9{margin-left:75%}.push-d-10{margin-left:83.3333333333%}.push-d-11{margin-left:91.6666666667%}.push-d-12{margin-left:100%}.pull-d-0{margin-right:0%}.pull-d-1{margin-right:8.3333333333%}.pull-d-2{margin-right:16.6666666667%}.pull-d-3{margin-right:25%}.pull-d-4{margin-right:33.3333333333%}.pull-d-5{margin-right:41.6666666667%}.pull-d-6{margin-right:50%}.pull-d-7{margin-right:58.3333333333%}.pull-d-8{margin-right:66.6666666667%}.pull-d-9{margin-right:75%}.pull-d-10{margin-right:83.3333333333%}.pull-d-11{margin-right:91.6666666667%}.pull-d-12{margin-right:100%}}@media(max-width: 768px){.col-t-0{width:0%}.col-t-1{width:8.3333333333%}.col-t-2{width:16.6666666667%}.col-t-3{width:25%}.col-t-4{width:33.3333333333%}.col-t-5{width:41.6666666667%}.col-t-6{width:50%}.col-t-7{width:58.3333333333%}.col-t-8{width:66.6666666667%}.col-t-9{width:75%}.col-t-10{width:83.3333333333%}.col-t-11{width:91.6666666667%}.col-t-12{width:100%}.push-t-0{margin-left:0%}.push-t-1{margin-left:8.3333333333%}.push-t-2{margin-left:16.6666666667%}.push-t-3{margin-left:25%}.push-t-4{margin-left:33.3333333333%}.push-t-5{margin-left:41.6666666667%}.push-t-6{margin-left:50%}.push-t-7{margin-left:58.3333333333%}.push-t-8{margin-left:66.6666666667%}.push-t-9{margin-left:75%}.push-t-10{margin-left:83.3333333333%}.push-t-11{margin-left:91.6666666667%}.push-t-12{margin-left:100%}.pull-t-0{margin-right:0%}.pull-t-1{margin-right:8.3333333333%}.pull-t-2{margin-right:16.6666666667%}.pull-t-3{margin-right:25%}.pull-t-4{margin-right:33.3333333333%}.pull-t-5{margin-right:41.6666666667%}.pull-t-6{margin-right:50%}.pull-t-7{margin-right:58.3333333333%}.pull-t-8{margin-right:66.6666666667%}.pull-t-9{margin-right:75%}.pull-t-10{margin-right:83.3333333333%}.pull-t-11{margin-right:91.6666666667%}.pull-t-12{margin-right:100%}}@media(max-width: 576px){.col-m-0{width:0%}.col-m-1{width:8.3333333333%}.col-m-2{width:16.6666666667%}.col-m-3{width:25%}.col-m-4{width:33.3333333333%}.col-m-5{width:41.6666666667%}.col-m-6{width:50%}.col-m-7{width:58.3333333333%}.col-m-8{width:66.6666666667%}.col-m-9{width:75%}.col-m-10{width:83.3333333333%}.col-m-11{width:91.6666666667%}.col-m-12{width:100%}.push-m-0{margin-left:0%}.push-m-1{margin-left:8.3333333333%}.push-m-2{margin-left:16.6666666667%}.push-m-3{margin-left:25%}.push-m-4{margin-left:33.3333333333%}.push-m-5{margin-left:41.6666666667%}.push-m-6{margin-left:50%}.push-m-7{margin-left:58.3333333333%}.push-m-8{margin-left:66.6666666667%}.push-m-9{margin-left:75%}.push-m-10{margin-left:83.3333333333%}.push-m-11{margin-left:91.6666666667%}.push-m-12{margin-left:100%}.pull-m-0{margin-right:0%}.pull-m-1{margin-right:8.3333333333%}.pull-m-2{margin-right:16.6666666667%}.pull-m-3{margin-right:25%}.pull-m-4{margin-right:33.3333333333%}.pull-m-5{margin-right:41.6666666667%}.pull-m-6{margin-right:50%}.pull-m-7{margin-right:58.3333333333%}.pull-m-8{margin-right:66.6666666667%}.pull-m-9{margin-right:75%}.pull-m-10{margin-right:83.3333333333%}.pull-m-11{margin-right:91.6666666667%}.pull-m-12{margin-right:100%}}@media(max-width: 1024px){.col-d-0{width:0%}.col-d-1{width:8.3333333333%}.col-d-2{width:16.6666666667%}.col-d-3{width:25%}.col-d-4{width:33.3333333333%}.col-d-5{width:41.6666666667%}.col-d-6{width:50%}.col-d-7{width:58.3333333333%}.col-d-8{width:66.6666666667%}.col-d-9{width:75%}.col-d-10{width:83.3333333333%}.col-d-11{width:91.6666666667%}.col-d-12{width:100%}.push-d-0{margin-left:0%}.push-d-1{margin-left:8.3333333333%}.push-d-2{margin-left:16.6666666667%}.push-d-3{margin-left:25%}.push-d-4{margin-left:33.3333333333%}.push-d-5{margin-left:41.6666666667%}.push-d-6{margin-left:50%}.push-d-7{margin-left:58.3333333333%}.push-d-8{margin-left:66.6666666667%}.push-d-9{margin-left:75%}.push-d-10{margin-left:83.3333333333%}.push-d-11{margin-left:91.6666666667%}.push-d-12{margin-left:100%}.pull-d-0{margin-right:0%}.pull-d-1{margin-right:8.3333333333%}.pull-d-2{margin-right:16.6666666667%}.pull-d-3{margin-right:25%}.pull-d-4{margin-right:33.3333333333%}.pull-d-5{margin-right:41.6666666667%}.pull-d-6{margin-right:50%}.pull-d-7{margin-right:58.3333333333%}.pull-d-8{margin-right:66.6666666667%}.pull-d-9{margin-right:75%}.pull-d-10{margin-right:83.3333333333%}.pull-d-11{margin-right:91.6666666667%}.pull-d-12{margin-right:100%}}@media(max-width: 768px){.col-t-0{width:0%}.col-t-1{width:8.3333333333%}.col-t-2{width:16.6666666667%}.col-t-3{width:25%}.col-t-4{width:33.3333333333%}.col-t-5{width:41.6666666667%}.col-t-6{width:50%}.col-t-7{width:58.3333333333%}.col-t-8{width:66.6666666667%}.col-t-9{width:75%}.col-t-10{width:83.3333333333%}.col-t-11{width:91.6666666667%}.col-t-12{width:100%}.push-t-0{margin-left:0%}.push-t-1{margin-left:8.3333333333%}.push-t-2{margin-left:16.6666666667%}.push-t-3{margin-left:25%}.push-t-4{margin-left:33.3333333333%}.push-t-5{margin-left:41.6666666667%}.push-t-6{margin-left:50%}.push-t-7{margin-left:58.3333333333%}.push-t-8{margin-left:66.6666666667%}.push-t-9{margin-left:75%}.push-t-10{margin-left:83.3333333333%}.push-t-11{margin-left:91.6666666667%}.push-t-12{margin-left:100%}.pull-t-0{margin-right:0%}.pull-t-1{margin-right:8.3333333333%}.pull-t-2{margin-right:16.6666666667%}.pull-t-3{margin-right:25%}.pull-t-4{margin-right:33.3333333333%}.pull-t-5{margin-right:41.6666666667%}.pull-t-6{margin-right:50%}.pull-t-7{margin-right:58.3333333333%}.pull-t-8{margin-right:66.6666666667%}.pull-t-9{margin-right:75%}.pull-t-10{margin-right:83.3333333333%}.pull-t-11{margin-right:91.6666666667%}.pull-t-12{margin-right:100%}}@media(max-width: 576px){.col-m-0{width:0%}.col-m-1{width:8.3333333333%}.col-m-2{width:16.6666666667%}.col-m-3{width:25%}.col-m-4{width:33.3333333333%}.col-m-5{width:41.6666666667%}.col-m-6{width:50%}.col-m-7{width:58.3333333333%}.col-m-8{width:66.6666666667%}.col-m-9{width:75%}.col-m-10{width:83.3333333333%}.col-m-11{width:91.6666666667%}.col-m-12{width:100%}.push-m-0{margin-left:0%}.push-m-1{margin-left:8.3333333333%}.push-m-2{margin-left:16.6666666667%}.push-m-3{margin-left:25%}.push-m-4{margin-left:33.3333333333%}.push-m-5{margin-left:41.6666666667%}.push-m-6{margin-left:50%}.push-m-7{margin-left:58.3333333333%}.push-m-8{margin-left:66.6666666667%}.push-m-9{margin-left:75%}.push-m-10{margin-left:83.3333333333%}.push-m-11{margin-left:91.6666666667%}.push-m-12{margin-left:100%}.pull-m-0{margin-right:0%}.pull-m-1{margin-right:8.3333333333%}.pull-m-2{margin-right:16.6666666667%}.pull-m-3{margin-right:25%}.pull-m-4{margin-right:33.3333333333%}.pull-m-5{margin-right:41.6666666667%}.pull-m-6{margin-right:50%}.pull-m-7{margin-right:58.3333333333%}.pull-m-8{margin-right:66.6666666667%}.pull-m-9{margin-right:75%}.pull-m-10{margin-right:83.3333333333%}.pull-m-11{margin-right:91.6666666667%}.pull-m-12{margin-right:100%}}.tns-outer{padding:0 !important}.tns-outer [hidden]{display:none !important}.tns-outer [aria-controls],.tns-outer [data-action]{cursor:pointer}.tns-slider{-webkit-transition:all 0s;-moz-transition:all 0s;transition:all 0s}.tns-slider>.tns-item{-webkit-box-sizing:border-box;-moz-box-sizing:border-box;box-sizing:border-box}.tns-horizontal.tns-subpixel{white-space:nowrap}.tns-horizontal.tns-subpixel>.tns-item{display:inline-block;vertical-align:top;white-space:normal}.tns-horizontal.tns-no-subpixel:after{content:"";display:table;clear:both}.tns-horizontal.tns-no-subpixel>.tns-item{float:left}.tns-horizontal.tns-carousel.tns-no-subpixel>.tns-item{margin-right:-100%}.tns-no-calc{position:relative;left:0}.tns-gallery{position:relative;left:0;min-height:1px}.tns-gallery>.tns-item{position:absolute;left:-100%;-webkit-transition:transform 0s,opacity 0s;-moz-transition:transform 0s,opacity 0s;transition:transform 0s,opacity 0s}.tns-gallery>.tns-slide-active{position:relative;left:auto !important}.tns-gallery>.tns-moving{-webkit-transition:all .25s;-moz-transition:all .25s;transition:all .25s}.tns-autowidth{display:inline-block}.tns-lazy-img{-webkit-transition:opacity .6s;-moz-transition:opacity .6s;transition:opacity .6s;opacity:.6}.tns-lazy-img.tns-complete{opacity:1}.tns-ah{-webkit-transition:height 0s;-moz-transition:height 0s;transition:height 0s}.tns-ovh{overflow:hidden}.tns-visually-hidden{position:absolute;left:-10000em}.tns-transparent{opacity:0;visibility:hidden}.tns-fadeIn{opacity:1;filter:alpha(opacity=100);z-index:0}.tns-normal,.tns-fadeOut{opacity:0;filter:alpha(opacity=0);z-index:-1}.tns-vpfix{white-space:nowrap}.tns-vpfix>div,.tns-vpfix>li{display:inline-block}.tns-t-subp2{margin:0 auto;width:310px;position:relative;height:10px;overflow:hidden}.tns-t-ct{width:2333.3333333%;width:-webkit-calc(100% * 70 / 3);width:-moz-calc(100% * 70 / 3);width:2333.3333333333%;position:absolute;right:0}.tns-t-ct:after{content:"";display:table;clear:both}.tns-t-ct>div{width:1.4285714%;width:-webkit-calc(100% / 70);width:-moz-calc(100% / 70);width:1.4285714286%;height:10px;float:left}.animate{animation:animateElement cubic-bezier(0.3, 0.45, 0.45, 0.95) .75s;animation-duration:.75s;animation-iteration-count:1;transition:transform .15s}@keyframes animateElement{0%{transform:translate(0px, 80px)}100%{transform:translate(0px, 0px)}}.resize-animation-stopper *{transition:none !important}*,*::after,*::before{box-sizing:border-box}body{font-family:"Nunito",Helvetica Neue,Helvetica,Arial,sans-serif;font-size:19px;line-height:1.5;color:#120f3e;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}body.is-in::after{visibility:hidden;opacity:0;pointer-events:none}body::after{content:"";position:fixed;left:0;top:0;width:100%;height:100%;background-color:#fff;z-index:15;transition:1s}@media only screen and (max-width: 576px){body{font-size:17px}}*::selection{color:#fff;background-color:rgba(66,53,208,.8)}h1,h2,h3,h4,h5,h6{font-family:"Nunito",Helvetica Neue,Helvetica,Arial,sans-serif;font-weight:800;line-height:1.2}h1{font-size:36px}h2{font-size:28px}h3{font-size:24px}h4{font-size:20px}h5{font-size:18px}h6{font-size:16px}blockquote{position:relative;margin-top:30px;padding:25px 0 15px 48px;font-size:32px;line-height:42px;font-weight:900;font-style:normal}blockquote::before{content:"“";position:absolute;z-index:-1;top:.3em;left:-0.03em;font-size:5.5em;color:#f5f5f5}blockquote p{margin-bottom:10px}blockquote cite{display:inline-block;margin-top:8px;font-size:16px;font-style:normal;color:#120f3e}blockquote cite:before{content:"—" " "}@media only screen and (max-width: 1024px){blockquote{font-size:24px;line-height:34px}}@media only screen and (max-width: 576px){blockquote{padding:25px 0 15px 25px;font-size:20px;line-height:30px;text-align:left}}pre{overflow:auto;padding:15px;margin-bottom:0;font-size:14px;white-space:pre-wrap;word-wrap:break-word;word-break:break-all}img,.lightense-wrap{max-width:100%;height:auto;vertical-align:middle}img img,.lightense-wrap img,.gallery img{border-radius:16px}img img.lightense-open,.lightense-wrap img.lightense-open,.gallery img.lightense-open{border-radius:0}img+em,.lightense-wrap+em,.gallery+em{display:block;margin-top:20px;font-size:14px;line-height:22px;font-style:normal;font-weight:normal;text-align:center;color:#120f3e}img+em a,.lightense-wrap+em a,.gallery+em a{border-bottom:2px solid #fff;transition:all .35s}img+em a:hover,.lightense-wrap+em a:hover,.gallery+em a:hover{color:#4235d0;border-color:rgba(0,0,0,0)}@media only screen and (max-width: 576px){img img,.lightense-wrap img,.gallery img{border-radius:8px}img+em,.lightense-wrap+em,.gallery+em{margin-top:12px}}a{text-decoration:none;color:#120f3e;transition:all .35s}a:hover{color:#4235d0}hr{position:relative;display:block;height:1px;margin:60px 0;border:0}hr::before{content:". . .";position:absolute;top:-6px;left:50%;transform:translateX(-50%);font-size:24px;line-height:0;color:#120f3e}.table-container{display:block;max-width:100%;overflow-x:auto}table{font-size:12px;color:#120f3e;width:100%;border-width:1px;border-color:#120f3e;border-collapse:collapse;color:var(--heading-font-color)}table th{padding:10px;font-size:16px;text-align:left;border:1px solid #120f3e;color:#fff;font-weight:700;background-color:#120f3e}table tr{background-color:#f0f0f0;transition:all .3s ease}table tr:nth-child(even){background-color:rgba(0,0,0,0)}table td{padding:10px;font-size:14px;border:1px solid #120f3e}.top-cover,.footer-cover{position:absolute;z-index:-1;width:100%;overflow:hidden}.top-cover .cover-image,.footer-cover .cover-image{position:relative;width:100%;height:100%}.top-cover .cover-image::after,.footer-cover .cover-image::after{content:"";position:relative;display:block;width:100%;height:100%}.top-cover .cover-image img,.footer-cover .cover-image img{position:absolute;top:0;left:0;height:100%;width:100%;object-fit:cover;-webkit-filter:blur(10px);filter:blur(10px)}@supports(-webkit-backdrop-filter: none) or (backdrop-filter: none){.top-cover .cover-image img,.footer-cover .cover-image img{-webkit-filter:none;filter:none}}.top-cover{top:0;height:480px}.top-cover .cover-image::after{will-change:transform;background:linear-gradient(180deg, rgba(255, 255, 255, 0.8) 0%, #fff 95%)}@supports(-webkit-backdrop-filter: none) or (backdrop-filter: none){.top-cover .cover-image::after{background:linear-gradient(180deg, rgba(255, 255, 255, 0.65) 0%, #fff 95%);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}.footer-cover{bottom:0;height:220px;border-top:1px solid #fff}.footer-cover .cover-image::after{will-change:transform;background:linear-gradient(0, rgba(255, 255, 255, 0.8) 0%, #fff 95%)}@supports(-webkit-backdrop-filter: none) or (backdrop-filter: none){.footer-cover .cover-image::after{background:linear-gradient(0, rgba(255, 255, 255, 0.65) 0%, #fff 95%);-webkit-backdrop-filter:blur(10px);backdrop-filter:blur(10px)}}.lazy{opacity:0;transition:.8s ease-in-out}.lazy.loaded{opacity:1}.header{height:auto;transition:all .35s}.header__inner{position:relative;display:flex;align-items:center;width:100%;padding:40px 16px;min-height:80px}@media only screen and (max-width: 1024px){.header__inner{align-items:baseline}}@media only screen and (max-width: 576px){.header__inner{padding:30px 16px}}.logo__link{font-family:"Nunito",Helvetica Neue,Helvetica,Arial,sans-serif;font-size:24px;line-height:26px;font-weight:900}.logo__link:hover{color:#4235d0}.logo__image{max-height:50px}.main-nav{margin-left:auto}@media only screen and (max-width: 1024px){.main-nav{position:fixed;top:0;left:0;right:0;bottom:0;z-index:-1;opacity:0;overflow:auto;background:#fff;transition:all .25s ease}.main-nav.is-open{z-index:999;opacity:1;transition:all .25s ease}.main-nav .nav__list{width:100%;max-width:360px;padding-left:40px}.main-nav .nav__list .nav__item{display:block;margin:0}.main-nav .nav__list .nav__item:last-child{margin-right:0}.main-nav .nav__list .nav__item .dropdown-menu{position:relative;left:0;padding:0;opacity:1;visibility:visible;transform:scale(1);box-shadow:none}.main-nav .nav__list .nav__item .dropdown-menu .nav__link{margin-left:8px;margin-bottom:16px;font-size:15px}.main-nav .nav__list .nav__item .dropdown-menu .nav__link::before{content:"– "}.main-nav .nav__list .nav__item .dropdown-menu .nav__link:last-child{margin-bottom:24px}.main-nav .nav__list .nav__item .nav__link{display:block;padding:0;margin-bottom:24px;font-size:18px;font-weight:800}.main-nav .nav__list .nav__item .nav__link:hover{background-color:inherit}.main-nav .nav__list .nav__item .nav__link .arrow-down{display:none}.main-nav .nav__list .nav__item .nav__link.active-link{color:#4235d0}.main-nav .nav__list .nav__item .nav__link.active-link::after{content:none}.main-nav .nav__list .nav__item .nav__link.cta-button{margin-left:-4px;margin-bottom:0}}.main-nav__box{display:flex;align-items:center}.main-nav__box .nav__icon-close{position:absolute;top:20px;right:40px;display:none;align-items:center;justify-content:center;width:36px;height:36px;font-size:24px;border-radius:50%;cursor:pointer;background-color:rgba(66,53,208,.08)}.main-nav__box .nav__icon-close:hover .ion-md-close{transform:rotate(90deg)}.main-nav__box .nav__icon-close .ion-md-close{color:#4235d0;transition:all .35s}.main-nav__box .nav__title{display:none}@media only screen and (max-width: 1024px){.main-nav__box{display:block;margin:24px 0}.main-nav__box .nav__icon-close{display:flex}.main-nav__box .nav__title{display:block;padding:0 0 24px 40px;margin-bottom:24px;font-size:32px;line-height:1;font-weight:900;color:#120f3e;border-bottom:1px solid #fff}}@media only screen and (max-width: 576px){.main-nav__box .nav__icon-close{right:30px}}.nav__list .nav__item{display:inline-block;margin:0 24px}.nav__list .nav__item:last-child{margin-right:0}.nav__list .nav__item .nav__link{position:relative;padding-bottom:10px;font-size:16px;line-height:1;font-weight:800;cursor:pointer}.nav__list .nav__item .nav__link:hover{color:#4235d0}.nav__list .nav__item .nav__link.active-link::after{content:"";position:absolute;left:50%;bottom:-6px;transform:translateX(-50%);display:block;width:6px;height:6px;border-radius:50%;background-color:#4235d0}.nav__list .nav__item .nav__link.cta-button{display:inline-flex;font-size:16px;padding:16px 24px}.nav__list .nav__item.dropdown{position:relative}.nav__list .nav__item.dropdown .arrow-down{vertical-align:middle}.nav__list .nav__item.dropdown:hover .dropdown-menu{opacity:1;visibility:visible;transform:scale(1)}.nav__list .dropdown-menu{position:absolute;top:calc(100% + 8px);left:-32px;transform:translateZ(0) scale(0.9);backface-visibility:hidden;z-index:100;min-width:200px;display:block;padding:20px;opacity:0;visibility:hidden;transition:all .25s ease;border-radius:8px;box-shadow:0px 20px 20px rgba(66,53,208,.07);background-color:#fff}.nav__list .dropdown-menu .nav__link{display:flex;padding:8px 12px;font-size:15px;border-radius:4px;will-change:transform}.nav__list .dropdown-menu .nav__link:hover{background-color:rgba(66,53,208,.07)}.nav__list .dropdown-menu .nav__link.active-link{color:#4235d0}.nav__list .dropdown-menu .nav__link.active-link::after{content:none}.nav-button{display:flex;align-items:center;font-size:24px;color:#120f3e;cursor:pointer}.nav-button .nav__icon{transition:all .35s}.nav-button .nav__icon:hover{color:#4235d0}.nav-button .nav__icon-menu{display:none}@media only screen and (max-width: 1024px){.nav-button{margin-left:auto}.nav-button .nav__icon-menu{display:block}}.pagination{margin:40px 0}@media only screen and (max-width: 768px){.pagination{margin-top:32px}}.pagination__inner{display:flex;justify-content:center;align-items:center}.pagination__list{display:flex;font-weight:800;height:48px;padding:0 20px;font-size:15px;line-height:48px;border-radius:16px;box-shadow:0px 0px 15px rgba(0,0,0,.05);color:#120f3e;background-color:#fff}.pagination__count{display:inline-block;padding:0 16px;margin:0 16px;font-size:14px;border-left:1px solid rgba(18,15,62,.1);border-right:1px solid rgba(18,15,62,.1);color:#58576d}.pagination__next i,.pagination__prev i{font-size:13px}.pagination__next:hover,.pagination__prev:hover{color:#4235d0}.pagination__next.disabled,.pagination__prev.disabled{opacity:.3;cursor:not-allowed;color:inherit}.pagination__next i{margin-left:4px}.pagination__prev i{margin-right:4px}.footer{position:relative;padding-top:96px;overflow:hidden}@media only screen and (max-width: 1024px){.footer{padding-top:80px}}@media only screen and (max-width: 576px){.footer{padding-top:60px}}.copyright{margin:20px 0 60px;font-size:14px;text-align:center;color:#58576d}.copyright a{font-weight:800}.copyright a:hover{color:#4235d0}.gallery-box{margin:40px 0}.gallery{display:grid;grid-template-columns:repeat(3, auto);justify-content:center;align-content:center;grid-gap:10px}.gallery .gallery__image{background:#fff}.gallery .gallery__image img{display:block;width:100%;height:auto;object-fit:cover}.social{text-align:center}.social .social__list{display:flex;justify-content:center;align-items:center;flex-wrap:wrap}.social .social__item{display:inline-block;margin:0 2px}.social .social__link{position:relative;z-index:1;padding:0 10px;font-size:21px;line-height:1}.social .social__link:hover{color:#fff}.social .social__link:hover::after{transform:translate3d(-50%, -50%, 0) scale(1)}.social .social__link::after{content:"";display:block;position:absolute;z-index:-1;top:50%;left:50%;transform:translate3d(-50%, -50%, 0) scale(0);width:100%;height:0;padding-bottom:100%;border-radius:50%;background:#4235d0;transition:all .25s}.author{padding:96px 0}@media only screen and (max-width: 1024px){.author{padding:80px 0}}@media only screen and (max-width: 768px){.author{padding:40px 0 80px}}@media only screen and (max-width: 576px){.author{padding:30px 0 60px}}.author__inner{display:flex;align-items:center}@media only screen and (max-width: 768px){.author__inner{flex-direction:column}}.author__left,.author__right{width:50%}@media only screen and (max-width: 768px){.author__left,.author__right{width:100%}}.author__left{max-width:520px;padding-right:20px;margin-right:auto}.author__left .author__name{font-size:56px;font-weight:900;color:#120f3e}@media only screen and (max-width: 1130px){.author__left .author__name{font-size:44px;line-height:1.2}}@media only screen and (max-width: 1024px){.author__left .author__name{font-size:36px;margin-bottom:24px}}@media only screen and (max-width: 576px){.author__left .author__name{font-size:32px}}.author__left .author__bio{margin-bottom:0;font-size:19px;line-height:1.6;color:#120f3e}@media only screen and (max-width: 1024px){.author__left .author__bio{font-size:17px}}.author__left .author__btn{margin-top:44px;display:inline-flex;justify-content:revert;align-items:center;font-size:12px;text-transform:uppercase;line-height:1;font-weight:800;padding:4px 12px;border-radius:30px;border:1px solid #4235d0;color:#4235d0;outline:none;cursor:pointer;transition:all .35s}.author__left .author__btn:hover{background-color:rgba(66,53,208,.08)}.author__left .author__btn:hover .circle-bg{transition:all .2s cubic-bezier(0.215, 0.61, 0.355, 1);transform:scale(0.94)}.author__left .author__btn .circle-bg{display:flex;align-items:center;justify-content:center;width:24px;height:24px;margin-right:8px;border-radius:50%;will-change:transform;background:#4235d0;transition:all .2s cubic-bezier(0.215, 0.61, 0.355, 1)}.author__left .author__btn .button-arrow{color:#fff}@media only screen and (max-width: 1024px){.author__left .author__btn{margin-top:36px}}.author__right{position:relative}.author__right .author__image{position:relative;z-index:10;width:480px;height:480px;margin-left:auto}.author__right .author__image::before{content:"";display:block;position:absolute;top:0;z-index:1;width:100%;height:100%;border-radius:90px;background:#fff}.author__right .author__image img{position:absolute;top:0;z-index:1;height:100%;width:100%;border-radius:90px;object-fit:cover;user-select:none;box-shadow:0px 100px 80px rgba(98,56,189,.07),0px 41.7776px 33.4221px rgba(98,56,189,.0503198),0px 22.3363px 17.869px rgba(98,56,189,.0417275),0px 12.5216px 10.0172px rgba(98,56,189,.035),0px 6.6501px 5.32008px rgba(98,56,189,.0282725),0px 2.76726px 2.21381px rgba(98,56,189,.0196802);transition:inherit;background:#fff}@media only screen and (max-width: 1130px){.author__right .author__image{width:400px;height:400px}}@media only screen and (max-width: 1024px){.author__right .author__image{width:340px;height:340px}.author__right .author__image img{border-radius:60px}}@media only screen and (max-width: 768px){.author__right{order:-1}.author__right .author__image{width:100%;height:380px;margin-bottom:40px}.author__right .author__image img{border-radius:30px}}@media only screen and (max-width: 576px){.author__right .author__image{height:280px;margin-bottom:32px}.author__right .author__image img{border-radius:16px;box-shadow:none}}.section{padding:48px 0}@media only screen and (max-width: 1024px){.section{padding:40px 0}}@media only screen and (max-width: 576px){.section{padding:32px 0}}.section__info{margin-bottom:60px}.section__info .section__head{display:flex;justify-content:space-between;align-items:center}.section__info .section__head .section__title{margin:0;font-size:40px;font-weight:900}.section__info .section__head .section__link{font-size:14px;font-weight:800;line-height:1;text-transform:uppercase;white-space:nowrap;will-change:transform}.section__info .section__head .section__link:hover{color:#4235d0}.section__info .section__head .section__link:hover i{transform:translateX(2px)}.section__info .section__head .section__link i{margin-left:4px;transition:.25s}.section__info .section__description{max-width:450px;line-height:1.6;margin:20px 0 0}@media only screen and (max-width: 1024px){.section__info .section__description{max-width:100%}}@media only screen and (max-width: 576px){.section__info{margin-bottom:48px}.section__info .section__head .section__title{margin-right:12px;font-size:28px}.section__info .section__head .section__link{font-size:13px}}.text-span{background:linear-gradient(135deg, #4235d0 38%, #f969cd 100%);-webkit-background-clip:text;-webkit-text-fill-color:rgba(0,0,0,0)}.project{margin-bottom:32px;transition:transform .2s ease}.project:hover .project__image::before{opacity:1;visibility:visible}@supports(-webkit-backdrop-filter: none) or (backdrop-filter: none){.project:hover .project__image::before{-webkit-backdrop-filter:blur(2px);backdrop-filter:blur(2px)}}.project:hover .project__image img{filter:blur(2px)}@supports(-webkit-backdrop-filter: none) or (backdrop-filter: none){.project:hover .project__image img{-webkit-filter:none;filter:none}}.project:hover .project__info .project__title,.project:hover .project__info .project__subtitle{opacity:1;transform:translateZ(0);transition:transform .4s cubic-bezier(0.165, 0.85, 0.45, 1) .2s,opacity .4s cubic-bezier(0.165, 0.85, 0.45, 1) .2s}@media only screen and (max-width: 768px){.project:last-child{margin-bottom:0}}.project__content{position:relative;display:flex;width:100%;height:100%;min-height:280px;border-radius:30px;transform:translate(0);overflow:hidden}.project__content::after{content:"";display:table;padding-top:80%}.project__content .project__image{position:absolute;display:block;width:100%;height:100%;user-select:none;background-color:#fff}.project__content .project__image::before{content:"";position:absolute;top:0;left:0;z-index:1;width:100%;height:100%;transition:all .4s ease 0s;opacity:0;visibility:hidden;background:rgba(18,15,62,.5)}.project__content .project__image img{position:absolute;top:0;left:0;width:100%;height:100%;object-fit:cover;border-radius:30px;background-color:#fff;pointer-events:none;transition:all .4s ease 0s}.project__content .project__info{z-index:1;width:100%;margin-top:auto;padding:0 32px 32px;pointer-events:none}.project__content .project__info .project__info-wrap{overflow:hidden}.project__content .project__info .project__title{font-size:28px;font-weight:900;line-height:1;margin-bottom:0;transform:translate3d(0, 100%, 0)}.project__content .project__info .project__subtitle{font-size:16px;opacity:0;transform:translate3d(0, -100%, 0)}.project__content .project__info .project__title,.project__content .project__info .project__subtitle{color:#fff;opacity:0;transition:transform .4s cubic-bezier(0.165, 0.85, 0.45, 1),opacity .4s cubic-bezier(0.165, 0.85, 0.45, 1)}@media only screen and (max-width: 576px){.project__content{border-radius:16px}.project__content .project__image img{border-radius:16px}}.testimonial-item{position:relative}.testimonial-item .testimonial-content{padding:0px 36px;border-radius:30px}.testimonial-item .testimonial-content .client-meta{display:flex;align-items:center}.testimonial-item .testimonial-content .client-text{margin-top:16px;margin-bottom:0px;font-size:16px}.testimonial-item .testimonial-content .image-container{position:relative;width:60px;height:60px;margin-right:12px;border-radius:50%;overflow:hidden;box-shadow:0px 100px 80px rgba(0,0,0,.07),0px 41.7776px 33.4221px rgba(0,0,0,.0503198),0px 22.3363px 17.869px rgba(0,0,0,.0417275),0px 12.5216px 10.0172px rgba(0,0,0,.035),0px 6.6501px 5.32008px rgba(0,0,0,.0282725),0px 2.76726px 2.21381px rgba(0,0,0,.0196802)}.testimonial-item .testimonial-content .image-container img{position:absolute;top:0;left:0;height:100%;width:100%;padding:2px;border-radius:50%;border:2px solid #fff;object-fit:cover}.testimonial-item .testimonial-content .client-info{display:flex;flex-direction:column}.testimonial-item .testimonial-content .client-name{margin-bottom:4px;font-size:18px}.testimonial-item .testimonial-content .client-designation{margin:0;font-size:14px;line-height:1.2}@media only screen and (max-width: 576px){.testimonial-item .testimonial-content{border-radius:16px}}.tns-outer{position:relative}.controls{display:flex;align-items:center;outline:none}.controls .prev,.controls .next{display:flex;align-items:center;justify-content:center;width:32px;height:32px;margin:0;padding:0;font-size:16px;border-radius:50%;border:1px solid #4235d0;color:#fff;background-color:#4235d0;outline:none;cursor:pointer;transition:all .2s ease}.controls .prev:hover,.controls .next:hover{color:#4235d0;background-color:rgba(0,0,0,0)}.controls .prev:active,.controls .next:active{transform:scale(0.97) translateY(1px)}.controls .prev{margin-right:4px}.article{margin-bottom:32px;will-change:transform;transition:transform .2s}.article:hover{transform:translateY(-3px)}@media only screen and (max-width: 768px){.article:last-child{margin-bottom:0}}.article__inner{border-radius:16px;overflow:hidden;transform:translate(0);box-shadow:0px 5px 20px rgba(0,0,0,.15)}.figure__inner{border-radius:16px;overflow:hidden;transform:translate(0)}.image-wrap{background:#fff}.article__image{position:relative;display:block;height:0;padding-bottom:66.25%;background:#fff;overflow:hidden}.article__image img{position:absolute;width:100%;height:100%;object-fit:cover;background-color:#fff}.figure__image{position:relative;display:block;margin:1rem;height:0;padding-bottom:66.25%;background:#fff;overflow:hidden}.figure__image img{position:absolute;width:100%;height:100%;background-color:#fff}.article__content{padding:28px;background:#fff}.figure__content{padding:4px;background:#fff}.article-tags__box{display:flex}.article-tags__box .article__tag{position:relative;z-index:1;display:inline-block;padding:8px 12px;margin:0 4px 4px 0;font-size:12px;line-height:10px;font-weight:800;text-transform:capitalize;border-radius:16px;color:#fff;background:linear-gradient(90deg, #4235d0 0%, #f969cd 167%)}.article-tags__box .article__tag:hover:before{opacity:1}.article-tags__box .article__tag::before{content:"";position:absolute;top:0;left:0;z-index:-1;display:block;width:100%;height:100%;opacity:0;border-radius:16px;will-change:transform;transition:opacity .35s;background:linear-gradient(90deg, #4235d0 0%, #f969cd 107%)}.article-tags__box .article__tag:last-child{margin:0 0 4px 0}.article__title{margin:16px 0;font-size:22px;line-height:1.3;font-weight:900}.article__title a:hover{color:#120f3e}.figure__title{margin:16px 0;font-size:22px;line-height:1.3;font-weight:900;text-align:center}.figure__title a:hover{color:#120f3e}.article__excerpt{display:-webkit-box;font-size:16px;line-height:24px;margin-bottom:20px;color:#58576d;overflow-y:hidden;-webkit-line-clamp:2;-webkit-box-orient:vertical}.figure__excerpt{display:-webkit-box;font-size:16px;line-height:24px;margin-bottom:20px;color:#58576d;overflow-y:hidden;-webkit-line-clamp:2;-webkit-box-orient:vertical}.article__meta{display:flex;align-items:center;flex-wrap:wrap}.article__author-image{position:relative;width:48px;height:48px;margin-right:8px;border-radius:50%;overflow:hidden}.article__author-image img{position:absolute;top:0;left:0;height:100%;width:100%;border-radius:50%;object-fit:cover}.article-info{display:flex;flex-direction:column}.article-info .article__author-link{margin-bottom:2px;font-size:16px;font-weight:800}.article-info .article__author-link:hover{color:#120f3e}.article-info .article__date{font-size:12px;line-height:20px;color:#58576d}.contact-modal{position:fixed;top:0;left:0;right:0;bottom:0;z-index:-1;overflow:auto;opacity:0;background:#fff;transition:all .25s ease}.contact-modal.is-visible{z-index:999;opacity:1;transition:all .25s ease}.contact-modal .container{position:relative}.contact-close{position:absolute;top:40px;right:16px;display:flex;align-items:center;justify-content:center;text-align:center;width:36px;height:36px;font-size:24px;border-radius:50%;cursor:pointer;background-color:rgba(66,53,208,.08)}.contact-close:hover .ion-md-close{transform:rotate(90deg)}.contact-close .ion-md-close{color:#4235d0;transition:all .35s}.contact-close .ion-md-close:hover{color:#4235d0}.contact-head{margin-bottom:48px;text-align:center}.contact-title{position:relative;display:inline-block;margin-bottom:20px;font-size:52px;font-weight:900}.contact-title::after{content:"";position:absolute;z-index:-1;top:-42%;left:-16%;transform:rotate(-45deg);display:block;width:130px;height:100px;background-image:-webkit-repeating-radial-gradient(center center, rgba(66, 53, 208, 0.7), rgba(66, 53, 208, 0.7) 2px, transparent 2px, transparent 100%);background-image:-moz-repeating-radial-gradient(center center, rgba(66, 53, 208, 0.7), rgba(66, 53, 208, 0.7) 2px, transparent 2px, transparent 100%);background-image:-ms-repeating-radial-gradient(center center, rgba(66, 53, 208, 0.7), rgba(66, 53, 208, 0.7) 2px, transparent 2px, transparent 100%);background-image:repeating-radial-gradient(center center, rgba(66, 53, 208, 0.7), rgba(66, 53, 208, 0.7) 2px, transparent 2px, transparent 100%);-webkit-background-size:15px 15px;-moz-background-size:15px 15px;background-size:15px 15px;opacity:.8;user-select:none}.contact-description{font-size:20px;line-height:1.4;font-weight:800}@media only screen and (max-width: 576px){.contact-description{font-size:16px}}.form-box{max-width:580px;margin:0 auto;padding-top:140px}.form{position:relative;margin-bottom:60px}.form__group{margin-bottom:20px}.form__group.form__group-button{position:absolute;bottom:-5%;left:50%;transform:translateX(-50%);margin-bottom:0}.form__group.form__group-button button{box-shadow:0px 100px 80px rgba(98,56,189,.07),0px 41.7776px 33.4221px rgba(98,56,189,.0503198),0px 22.3363px 17.869px rgba(98,56,189,.0417275),0px 12.5216px 10.0172px rgba(98,56,189,.035),0px 6.6501px 5.32008px rgba(98,56,189,.0282725),0px 2.76726px 2.21381px rgba(98,56,189,.0196802)}.form__group.form__group-button button i{margin-left:8px}.form__input{width:100%;padding:20px;font-size:16px;font-weight:700;border:4px solid #fff;border-radius:30px;outline:none;transition:all .35s;resize:vertical;color:#120f3e;background:#fff}.form__input::placeholder{color:#d9d9d9}.form__input:focus{border-color:rgba(66,53,208,.8)}.subscribe{padding-bottom:0}.subscribe__inner{max-width:580px;margin:0 auto}.subscribe__info{margin-bottom:40px;text-align:center}.subscribe__info .subscribe__title{position:relative;display:inline-block;margin-bottom:32px;font-size:56px;font-weight:900}.subscribe__info .subscribe__title::after{content:"";position:absolute;z-index:-1;top:-34%;left:-9%;transform:rotate(-45deg);display:block;width:130px;height:100px;background-image:-webkit-repeating-radial-gradient(center center, rgba(66, 53, 208, 0.7), rgba(66, 53, 208, 0.7) 2px, transparent 2px, transparent 100%);background-image:-moz-repeating-radial-gradient(center center, rgba(66, 53, 208, 0.7), rgba(66, 53, 208, 0.7) 2px, transparent 2px, transparent 100%);background-image:-ms-repeating-radial-gradient(center center, rgba(66, 53, 208, 0.7), rgba(66, 53, 208, 0.7) 2px, transparent 2px, transparent 100%);background-image:repeating-radial-gradient(center center, rgba(66, 53, 208, 0.7), rgba(66, 53, 208, 0.7) 2px, transparent 2px, transparent 100%);-webkit-background-size:15px 15px;-moz-background-size:15px 15px;background-size:15px 15px;opacity:.8;user-select:none}.subscribe__info .subscribe__subtitle{margin:0 auto;max-width:430px;font-size:28px;font-weight:800;line-height:1.2}@media only screen and (max-width: 576px){.subscribe__info .subscribe__title{font-size:36px}.subscribe__info .subscribe__title::after{top:-64%;left:-11%}.subscribe__info .subscribe__subtitle #ityped{font-size:20px}}@media only screen and (max-width: 360px){.subscribe__info .subscribe__title::after{top:-34%;left:-9%}}.subscribe-form{display:flex;flex-direction:column;max-width:360px;margin:0 auto}.subscribe-form .subscribe-email{padding:18px;margin-bottom:20px;font-weight:800;text-align:center;border:none;border-radius:50px;border:4px solid #fff;outline:none;color:#120f3e;background:rgba(0,0,0,0);transition:border-color .2s ease-in-out}.subscribe-form .subscribe-email::placeholder{color:#d9d9d9}.subscribe-form .subscribe-email:focus{border-color:rgba(66,53,208,.7)}.ityped-cursor{font-weight:400;animation:blink .4s infinite;animation-direction:alternate}@keyframes blink{0%,to{color:rgba(0,0,0,0)}100%{color:#4235d0}}@-webkit-keyframes blink{0%,to{color:rgba(0,0,0,0)}100%{color:#4235d0}}.button{position:relative;z-index:1;display:inline-flex;justify-content:center;line-height:1;font-weight:800;border-radius:30px;white-space:nowrap;border:none;outline:none;cursor:pointer;overflow:hidden;color:#fff;background:linear-gradient(90deg, #4235d0 0%, #f969cd 167%);transition:all .25s ease}.button:hover{color:#fff !important}.button:hover:before{opacity:1}.button::before{content:"";position:absolute;top:0;left:0;z-index:-1;display:block;width:100%;height:100%;opacity:0;will-change:transform;transition:opacity .35s;background:linear-gradient(90deg, #4235d0 0%, #f969cd 107%)}.button:active{transform:translateY(2px) scale(0.98)}.button--small{font-size:15px;padding:16px 24px}.button--middle{min-width:200px;font-size:18px;padding:20px}.button--big{width:100%;padding:20px;font-size:18px}.top{position:fixed;bottom:40px;right:-100px;z-index:1;width:36px;height:36px;font-size:18px;line-height:36px;text-align:center;border-radius:50%;background-color:#fff;color:#120f3e;cursor:pointer;transition:all .25s ease;box-shadow:0px 0px 20px rgba(0,0,0,.07)}.top:hover{color:#fff;background-color:#120f3e}.top.is-active{right:40px}@media only screen and (max-width: 768px){.top{bottom:25px}.top.is-active{right:30px}}.post-head,.page-head{max-width:780px;margin:0 auto;padding:96px 0 60px;text-align:center}@media only screen and (max-width: 1024px){.post-head,.page-head{padding:80px 0 60px}}@media only screen and (max-width: 768px){.post-head,.page-head{padding:40px 0}}@media only screen and (max-width: 576px){.post-head,.page-head{padding:30px 0}}.post-title,.page-title{margin:0;font-size:56px;font-weight:900;line-height:1.12}@media only screen and (max-width: 768px){.post-title,.page-title{font-size:36px}}@media only screen and (max-width: 576px){.post-title,.page-title{font-size:32px}}.page-description{margin:30px auto 0;max-width:560px;line-height:1.6}@media only screen and (max-width: 576px){.page-description{margin:24px auto 0}}.post-image,.page-image{position:relative;margin-bottom:48px;border-radius:30px;overflow:hidden}.post-image::after,.page-image::after{content:"";display:block;min-height:280px;padding-top:56.25%;background-color:#fff}.post-image img,.page-image img{position:absolute;top:0;width:100%;height:100%;border-radius:30px;background-color:#fff;object-fit:cover;user-select:none}@media only screen and (max-width: 768px){.post-image,.page-image{margin-bottom:40px}.post-image::after,.page-image::after{padding-top:70%}}@media only screen and (max-width: 576px){.post-image,.page-image{margin-bottom:30px;border-radius:16px}.post-image img,.page-image img{border-radius:16px}}.page-image404{position:relative;display:flex;justify-content:center;margin-bottom:48px;border-radius:30px;overflow:hidden}.page-image404::after{content:"";display:block;min-height:280px;padding-top:56.25%;background-color:#fff}.page-image404 img{position:absolute;top:0;height:100%;border-radius:30px;background-color:#fff;object-fit:cover;user-select:none}@media only screen and (max-width: 768px){.page-image404{margin-bottom:40px}.page-image404::after{padding-top:70%}}@media only screen and (max-width: 576px){.page-image404{margin-bottom:30px;border-radius:16px}.page-image404 img{border-radius:16px}}.post,.page,.project-content{max-width:780px;margin:0 auto;color:#120f3e}.post__content img,.page__content img,.project-content img,.js-reframe{border-radius:16px;overflow:hidden}@media only screen and (max-width: 768px){.post__content img,.page__content img,.project-content img,.js-reframe{border-radius:8px}}.post__content a,.page__content a,.project-content a{border-bottom:2px solid #fff;transition:all .35s}.post__content a:hover,.page__content a:hover,.project-content a:hover{color:#4235d0;border-color:rgba(0,0,0,0)}.post-tags__box{display:flex;justify-content:center;margin-bottom:16px}.post-tags__box .post__tag{position:relative;z-index:1;display:inline-block;padding:8px 12px;margin:0 4px 4px 0;font-size:12px;line-height:10px;font-weight:800;text-transform:capitalize;border-radius:16px;color:#fff;background:linear-gradient(90deg, #4235d0 0%, #f969cd 167%)}.post-tags__box .post__tag:hover:before{opacity:1}.post-tags__box .post__tag::before{content:"";position:absolute;top:0;left:0;z-index:-1;display:block;width:100%;height:100%;opacity:0;border-radius:16px;will-change:transform;transition:opacity .35s;background:linear-gradient(90deg, #4235d0 0%, #f969cd 107%)}.post-tags__box .post__tag:last-child{margin:0 0 4px 0}.post__info{display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap;margin-bottom:20px}@media only screen and (max-width: 576px){.post__info{margin-bottom:12px}}.post__author{display:flex;align-items:center;flex-wrap:wrap;margin:0 12px 12px 0}.post__author-image{position:relative;width:48px;height:48px;margin-right:8px;border-radius:50%;overflow:hidden}.post__author-image img{position:absolute;top:0;left:0;height:100%;width:100%;border-radius:50%;object-fit:cover;background-color:#fff}.post__author-info{display:flex;flex-direction:column}.post__author-link{margin-bottom:2px;font-size:16px;font-weight:800}.post__author-link:hover{color:#120f3e}.post__date{font-size:12px;line-height:20px;color:#58576d}.post__share{margin-bottom:12px}.post__share .share__list{display:flex;justify-content:center;align-items:center;width:100%}.post__share .share__item{margin:0 2px;text-align:center}.post__share .share__link{display:block;width:36px;height:36px;font-size:16px;line-height:36px;border-radius:50%;color:#fff}.post__share .share__link.share__facebook,.post__share .share__link.share__twitter,.post__share .share__link.share__linkedin,.post__share .share__link.share__pinterest{border:1px solid #fff;background:#fff}.post__share .share__link.share__facebook:hover,.post__share .share__link.share__twitter:hover,.post__share .share__link.share__linkedin:hover,.post__share .share__link.share__pinterest:hover{color:#fff}.post__share .share__link.share__facebook{color:#3b5998}.post__share .share__link.share__facebook:hover{background:#3b5998}.post__share .share__link.share__twitter{color:#55acee}.post__share .share__link.share__twitter:hover{background:#55acee}.post__share .share__link.share__linkedin{color:#0077b5}.post__share .share__link.share__linkedin:hover{background:#0077b5}.post__share .share__link.share__pinterest{color:#bd081c}.post__share .share__link.share__pinterest:hover{background:#bd081c}.post__navigation{display:flex;justify-content:space-between;margin:48px 0 96px}.post__navigation .post__prev,.post__navigation .post__next{display:flex;width:48%;margin-bottom:24px;background:#fff;transition:transform .2s;will-change:transform}.post__navigation .post__prev:hover,.post__navigation .post__next:hover{color:#120f3e;transform:translateY(-3px)}.post__navigation .post__prev .prev__image,.post__navigation .post__prev .next__image,.post__navigation .post__next .prev__image,.post__navigation .post__next .next__image{position:relative;display:block;width:100px;min-width:100px;height:100px;border-radius:50%;overflow:hidden;transform:translate(0)}.post__navigation .post__prev .prev__image img,.post__navigation .post__prev .next__image img,.post__navigation .post__next .prev__image img,.post__navigation .post__next .next__image img{position:absolute;top:0;left:0;width:100%;height:100%;background-color:#fff;object-fit:cover}.post__navigation .post__prev .prev__image,.post__navigation .post__next .prev__image{margin-right:16px}.post__navigation .post__prev .next__image,.post__navigation .post__next .next__image{margin-left:16px}.post__navigation .post__prev .post__nav,.post__navigation .post__next .post__nav{display:inline-block;margin-bottom:6px;font-size:14px;line-height:1;color:#58576d}.post__navigation .post__prev .post__nav__title,.post__navigation .post__next .post__nav__title{margin-bottom:0;font-size:20px}.post__navigation .post__next{text-align:right;margin-left:auto;justify-content:flex-end}@media only screen and (max-width: 768px){.post__navigation{flex-wrap:wrap;margin-top:40px}.post__navigation .post__prev,.post__navigation .post__next{width:100%}.post__navigation .post__prev{margin-bottom:24px}.post__navigation .next__box{margin-left:auto}}@media only screen and (max-width: 1024px){.post__navigation{margin:48px 0 80px}}@media only screen and (max-width: 576px){.post__navigation{margin:48px 0 60px}.post__navigation .post__prev .post__nav__title,.post__navigation .post__next .post__nav__title{font-size:18px}}.related-posts{display:none}.related-posts.is-related{display:block;padding-top:0}.related-title{position:relative;margin-bottom:48px;font-size:14px;text-transform:uppercase;text-align:center}.related-title::before{content:"";position:absolute;top:50%;left:0;transform:translateY(-50%);z-index:-1;display:block;width:100%;height:1px;background-color:#fff}.related-title span{padding:0 12px;background-color:#fff}.post__comments{max-width:780px;margin:0 auto 48px}@media only screen and (max-width: 576px){.post__comments{margin:0 auto 30px}}.error{margin-bottom:20px;text-align:center}.error .error__title{margin-bottom:20px;font-size:120px;line-height:1}.error .error__text{margin-bottom:40px;font-size:21px;font-weight:800}.error .error__button{border:none}.project-subtitle{margin:20px 0 0;font-size:20px;line-height:1;font-weight:800}.project__navigation{max-width:780px;padding:40px 0;margin:48px auto 60px;text-align:center;border-top:1px solid #fff;border-bottom:1px solid #fff}@media only screen and (max-width: 768px){.project__navigation{margin:40px auto}}.project__prev:hover{color:#120f3e}.project__prev:hover .project__nav__title .arrow-right{opacity:1;transform:translateX(3px)}.project__nav__info{display:inline-block;margin:0 20px 4px 0;font-size:16px;font-weight:800;text-transform:uppercase;color:#d9d9d9}@media only screen and (max-width: 768px){.project__nav__info{font-size:18px}}.project__nav__title{margin-bottom:20px;font-size:40px;font-weight:900;line-height:1}.project__nav__title .arrow-right{margin-left:2px;font-size:30px;opacity:0;transition:.2s linear}@media only screen and (max-width: 768px){.project__nav__title{font-size:36px}}.tag .page-title{text-transform:capitalize}.tag .tag-meta{display:inline-block;margin-top:12px;font-size:16px;font-weight:800;text-transform:uppercase}.tag .tag-meta .tag-count{display:inline-flex;align-items:center;justify-content:center;padding:.5em;border-radius:50%;color:#fff;background-color:#4235d0}.tag .tag-meta .tag-count.count-padding-small{padding:.25em}.tag .tag-meta .tag-count::after{content:"";display:block;height:0;width:100%;padding-bottom:100%}.tag .tag-meta .tag-count .tag-number{height:0;margin-top:-1em;font-size:14px;line-height:1;overflow:visible}
  </style>
</head>

<body>

  

  
    <div class="top-cover">
      <div class="cover-image">
        <img class="lazy" data-src="images/llama3.1-swallow.png" alt="Swallow LLM">
      </div>
    </div>
  

  
<!-- begin header -->
<header class="header">
  <div class="container">
    <div class="row">
      <div class="header__inner">

        <div class="logo">
          <a class="logo__link" href="/">
          
            Swallow LLM
          
          </a>
        </div>

        <nav class="main-nav">
          <div class="main-nav__box">

            <div class="nav__icon-close">
              <i class="ion ion-md-close"></i>
            </div>

            <div class="nav__title">Menu</div>

            <ul class="nav__list list-reset">
              <li class="nav__item">
                <a href="/index.en.html" class="nav__link">Home</a>
              </li>
              
            
              <li class="nav__item dropdown">
                <span class="nav__link dropdown-toggle">Projects <i class="ion ion-ios-arrow-down arrow-down"></i></span>
                <div class="dropdown-menu">
                  
                    <a href="/llama3.1-swallow.en.html" class="nav__link active-link">Llama 3.1 Swallow</a>
                  
                    <a href="/llama3-swallow.en.html" class="nav__link">Llama 3 Swallow</a>
                  
                    <a href="/swallow-llama.en.html" class="nav__link">Swallow</a>
                  
                    <a href="/evaluation/index.en.html" class="nav__link">Japanese LLM Evaluation</a>
                  
                </div>
              </li>

              
            
              <li class="nav__item">
                <a href="/blog/" class="nav__link">Blog</a>
              </li>
              
            

            
            <li class="nav__item">
              <a class="nav__link" href="/llama3.1-swallow.ja.html">日本語</a>
            </li>
            
      
            </ul>
          </div>
        </nav>

        <div class="nav-button">
          <i class="nav__icon nav__icon-menu ion ion-md-menu"></i>
        </div>

      </div>
    </div>
  </div>
</header>
<!-- end header -->

  <!-- begin content -->
  <main class="content animate" aria-label="Content">
    
<!-- begin author -->
<section class="author">
    <div class="container">
      <div class="row">
        <div class="col col-12">
          <div class="author__inner">
  
            <div class="author__left">
              <h1 class="author__name">Llama 3.1 Swallow</h1>
              <p class="author__bio"><p>Llama 3.1 Swallow is large language models (8B, 70B) that enhance Japanese language capabilities while maintaining the English abilities of Llama 3.1. The model parameters (weights) are available on HuggingFace, allowing usage for research and commercial purposes as long as it complies with the <a href="https://www.llama.com/llama3_1/license/">Llama 3.1 license</a> (uses of instruction-tuned models must comply with the <a href="https://www.llama.com/llama3_1/license/">Meta Llama 3.1 Community License</a> and must not violate the <a href="https://ai.google.dev/gemma/terms#3.2-use">Use Restriction</a> set forth in the <a href="https://ai.google.dev/gemma/terms">Gemma Terms of Use</a>). Llama 3.1 Swallow is based on Meta Llama 3.1 and was developed by the research team at the Okazaki Laboratory and Yokota Laboratory of the Institute of Science Tokyo, and the National Institute of Advanced Industrial Science and Technology (AIST). Built with Llama. (Released v0.1 on Oct 8, 2024; released v0.2 on Nov 11; released 8B v0.3 on Dec 23; released 70B v0.3 on Dec 30.)</p>
</p>
              
              <a class="author__btn" href="https://huggingface.co/collections/tokyotech-llm/llama-31-swallow-66fd4f7da32705cadd1d5bc6">
                <span class="circle-bg"><i class="ion ion-md-arrow-down button-arrow"></i></span> View on HuggingFace
              </a>
              
            </div>

            <div class="author__right">
              <div class="author__image">
                <img class="lazy" data-src="https://swallow-llm.github.io/images/llama3.1-swallow.png" alt="Llama 3.1 Swallow" />
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>
<p><!-- end author --></p>

<!-- begin section -->
<section class="section projects" id="projects">
    <div class="container">
      <div class="row">
        <div class="col col-12">
          <div class="contaniner__inner animate">
            <div class="section__info">
              <div class="section__head">
                
                <a name="update">
                
                <h2 class="section__title"><span class="text-span">History</span></h2>
                
                </a>
                
              </div>
            </div>
          </div>
          <div>
            
<ul>
  <li>2024-12-30: Released Llama 3.1 Swallow 70B Instruct v0.3 and updated the website. This version was instruction-tuned using the same dataset as Llama 3.1 Swallow 8B Instruct v0.3, improving Japanese multi-turn dialogue capabilities. The difference between v0.1 and v0.3 lies solely in instruction tuning, as both share the same base model, Llama 3.1 Swallow 70B v0.1. (As a result, v0.2 and the base model’s v0.3 are skipped). This release is a minor update that integrates the Swallow team’s latest insights into model construction. There are no issues with version 0.1.</li>
  <li>2024-12-23: Released Llama 3.1 Swallow 8B Instruct v0.3 and updated our website. In v0.3, instruction tuning was conducted using newly developed data, significantly improving Japanese multi-turn conversational capabilities. The average score on the Japanese MT-Bench improved by 8.4 points from v0.2 to v0.3, achieving a top-tier score of 0.6424 among models in this class. Furthermore, instruction-following capabilities in multi-turn conversations were enhanced. The only difference between v0.2 and v0.3 lies in the instruction tuning, as both share the same base model, Llama 3.1 Swallow 8B v0.2 (hence, there is no base model labeled as v0.3). This release is a minor update incorporating the latest insights from the Swallow team, and there are no issues with versions 0.1 or 0.2. Due to the termination of ABCI 2.0 operations at the end of October 2024, it has not been possible to develop new versions of Llama 3.1 Swallow 70B. If the opportunity arises, we plan to update the 70B model in the future.</li>
  <li>2024-11-11: Released Llama 3.1 Swallow 8B v0.2 and Llama 3.1 Swallow 8B Instruct v0.2, and updated the website description. Changes in v0.2 include improvements in the quality of training data for Japanese program code, and a review of the Japanese-English data ratio. Additionally, Llama 3.1 Swallow 8B Instruct v0.2 continues pre-training from Llama 3.1 8B Instruct (whereas v0.1 continued from the base model) and includes further instruction tuning. This minor update incorporates our latest insights into model construction and does not indicate any issues with v0.1. Due to the termination of ABCI 2.0 operations at the end of October 2024, we were unable to complete v0.2 of the Llama 3.1 Swallow 70B model. We hope to update the 70B model at a later date if possible.</li>
  <li>2024-10-08: Released Llama 3.1 Swallow.</li>
</ul>


          </div>
        </div>
      </div>
    </div>
</section>
<!-- end section -->

<!-- begin section -->
<section class="section projects" id="projects">
    <div class="container">
      <div class="row">
        <div class="col col-12">
          <div class="contaniner__inner animate">
            <div class="section__info">
              <div class="section__head">
                
                <a name="model">
                
                <h2 class="section__title"><span class="text-span">Models</span></h2>
                
                </a>
                
              </div>
            </div>
          </div>
          <div>
            
<p>Hugging Face</p>

<ul>
  <li>Llama 3.1 Swallow 8B Instruct v0.3: <a href="https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3">https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.3</a></li>
  <li>Llama 3.1 Swallow 70B Instruct v0.3: <a href="https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3">https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3</a></li>
  <li>Llama 3.1 Swallow 8B v0.2: <a href="https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2">https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.2</a></li>
  <li>Llama 3.1 Swallow 8B Instruct v0.2: <a href="https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2">https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.2</a></li>
  <li>Llama 3.1 Swallow 8B v0.1: <a href="https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1">https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-v0.1</a></li>
  <li>Llama 3.1 Swallow 8B Instruct v0.1: <a href="https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1">https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1</a></li>
  <li>Llama 3.1 Swallow 70B v0.1: <a href="https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1">https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-v0.1</a></li>
  <li>Llama 3.1 Swallow 70B Instruct v0.1: <a href="https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1">https://huggingface.co/tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.1</a></li>
</ul>

<p>NVIDIA NIM</p>

<ul>
  <li>Llama 3.1 Swallow 8B Instruct v0.1: <a href="https://build.nvidia.com/institute-of-science-tokyo/llama-3-1-swallow-8b-instruct-v01">https://build.nvidia.com/institute-of-science-tokyo/llama-3-1-swallow-8b-instruct-v01</a></li>
  <li>Llama 3.1 Swallow 70B Instruct v0.1: <a href="https://build.nvidia.com/institute-of-science-tokyo/llama-3-1-swallow-70b-instruct-v01">https://build.nvidia.com/institute-of-science-tokyo/llama-3-1-swallow-70b-instruct-v01 </a></li>
</ul>

<p>The license for Llama 3.1 Swallow inherits from the Meta <a href="https://www.llama.com/llama3_1/license/">Llama 3.1 license</a>. Additionally, for the use of the instruction-tuned models, Llama 3.1 Swallow 8B Instruct and Llama 3.1 Swallow 70B Instruct, users must not only comply with the Llama 3.1 License but also ensure that they do not violate the usage restrictions outlined in the <a href="https://ai.google.dev/gemma/terms">Gemma Terms of Use</a>. Provided the license and restrictions (applicable to the Instruct models only) are adhered to, research and commercial usage of these models are permitted.</p>


          </div>
        </div>
      </div>
    </div>
</section>
<!-- end section -->

<!-- begin section -->
<section class="section projects" id="projects">
    <div class="container">
      <div class="row">
        <div class="col col-12">
          <div class="contaniner__inner animate">
            <div class="section__info">
              <div class="section__head">
                
                <a name="evaluation">
                
                <h2 class="section__title"><span class="text-span">Performance</span></h2>
                
                </a>
                
              </div>
            </div>
          </div>
          <div>
            
<p>The Swallow team is advancing research and development with the goal of creating large language models (LLMs) with strong Japanese language capabilities. This is to elucidate the mechanisms and methods behind constructing LLMs that exhibit high abilities of language understanding, generation, and coversation. In addition to evaluating prototype LLMs developed by the research team, we also conduct evaluation experiments on LLMs created by other companies and research institutions to explore the “recipe” for developing excellent LLMs. From April 2024 to October, we have conducted over 400 experiments.</p>

<p>The Swallow project aims to build general LLMs model that can be easily fine-tuned for specific applications when necessary, rather than improving performance on specific tasks. For the 2024 fiscal year, the Swallow project employs question-answering tasks to test common knowledge, language generation tasks such as automatic summarization and machine translation, exams, and tasks requiring logical reasoning such as math and code generation. These evaluations are conducted with 10 datasets for Japanese understanding and generation tasks, and 9 datasets for English understanding and generation tasks. Additionally, to measure Japanese conversational ability, we conduct evaluations using a Japanese MT-Bench with GPT-4 as the judge.</p>

<p>For all tasks, evaluation scores range from 0 (minimum) to 1 (maximum). Notably, our Japanese MT-Bench evaluation results are observed to be lower than those on external leaderboards, even when our scores are scaled up tenfold to a 10-point system. While many external leaderboards use GPT-4 (gpt-4-0613) as a judge, we use GPT-4 (gpt-4-1106-preview), which is thought to be the cause of the score differences. Our investigations revealed that although there is a notable discrepancy between our results and those of external leaderboards, the rankings of the models remain mostly unchanged. Therefore, due to the substantial number of completed evaluations, we have chosen to continue with the version of GPT-4.</p>

<p>For more details on the evaluation, please refer to the <a href="/evaluation/about.en.html">Japanese LLM Evaluation</a>.</p>

<h2 id="8b-base">8B Base</h2>

<p>Since the performance of LLMs tends to increase with the number of parameters, it is essential to compare models of similar scale to evaluate the effectiveness of a model’s design recipe. However, comparing models of different scales can help users make informed choices, so we compared the performance of Llama 3.1 Swallow 8B against LLMs with fewer than 13B parameters. Below is a graph showing the average scores for Japanese understanding and generation tasks, sorted in descending order for base models with 13B parameters or fewer.</p>

<p style="text-align: center;">
  <img src="images/13b-base-en.svg" alt="Average scores of major base models under 13B on Japanese understanding and generation tasks" />
</p>

<p>The average score for Japanese understanding and generation tasks with Llama 3.1 Swallow 8B v0.2 is 0.4991, representing an increase of 2.74 points from the previous Llama 3 Swallow 8B version’s score of 0.4717, marking the highest score among open LLMs under 8B parameters. Additionally, Llama 3.1 8B achieved an average score of 0.4359, indicating a 6.32-point improvement in Japanese understanding and generation tasks through continual pre-training. Compared to Swallow 7B (released in December 2023), Swallow-MS 7B v0.1 (released in March 2024), and Llama 3 Swallow (released in July 2024), Llama 3.1 Swallow shows a steady increase in average scores.</p>

<p>When comparing the minor updates before and after (v0.1 and v0.2), JHumanEval improved by 5.49 points, and the question-answering task related to Japan (NIILC) rose by 2.61 points. We believe this improvement is due to our efforts in enhancing the quality of training data for Japanese and program code, and adjusting the Japanese-English data ratio. In addition, the average score for Japanese understanding and generation tasks increased by 0.86 points from 0.4905 of v0.1.</p>

<p>Looking beyond the Swallow models, Qwen2.5 7B and Gemma 2 9B also demonstrate impressive performance. These models are multilingual, making it notable that they perform well even without being specialized for Japanese. There is a nominal 1B parameter difference between Qwen2.5 7B and Llama 3.1 Swallow 8B, which highlights Qwen2.5 7B’s impressive capabilities. However, if we consider decimal precision, Qwen2.5 7B has 7.6B parameters, while Llama 3.1 Swallow 8B has 8.0B, meaning the actual size difference is only 0.4B, which may change this impression. Gemma 2 9B is the strongest among this class of models. At the same time, the difference of average scores between Llama 3.1 Swallow v0.2 and Gemma 2 9B is only 0.05 point, which indicates non distinguishable difference in performance. Since Gemma 2 9B has 9.2B parameters, Llama 3.1 Swallow 8B v0.2 competes well despite a 1.2B parameter disadvantage.</p>

<p>We visualized the scores of these three models on language understanding and generation tasks in Japanese and English using a radar chart. This chart reveals that Qwen2.5 excels in tasks involving mathematics (MGSM and GSM8K), general knowledge (JMMLU and MMLU), and code generation (JHumanEval and HumanEval), while Llama 3.1 Swallow is particularly strong in Japanese knowledge tasks (NIILC) and Japanese language generation tasks (WMT20 en-ja).</p>

<p style="text-align: center;">
  <img src="images/llama-3.1-swallow_gemma2_quwen2.5-en.png" alt="Radar chart of language understanding and generation task scores for Llama 3.1 Swallow 8B, Gemma 2 9B, and Qwen 2.5 7B" />
</p>

<h2 id="8b-instruct">8B Instruct</h2>

<p>Similarly, the graph below shows the average scores for Japanese understanding and generation tasks, Japanese MT-bench, and English understanding and generation tasks for major instruction-tuned models with 13B parameters or fewer. In this graph, the models are arranged by average scores on Japanese MT-bench scores.</p>

<p style="text-align: center;">
  <img src="images/13b-inst-en.svg" alt="Average benchmark scores for major instruction-tuned models under 13B" />
</p>

<p>The Japanese MT-Bench average score for Llama 3.1 Swallow 8B Instruct v0.3 is 0.6424, placing it among the top-performing Japanese LLMs with fewer than 13B parameters, second only to Gemma 2 9B IT, which scored 0.6753. This update marks the largest performance improvement (8.4 points) in the history of 8B Instruct models since Llama 3 Swallow, achieving a total improvement of 16.58 points compared to the Llama 3 Swallow 8B Instruct released approximately six months ago.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Japanese MT-Bench average score</th>
      <th>Release date</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Llama 3 Swallow 8B Instruct</td>
      <td>0.4766</td>
      <td>July 1, 2023</td>
    </tr>
    <tr>
      <td>Llama 3.1 Swallow 8B Instruct v0.1</td>
      <td>0.5327</td>
      <td>October 8, 2024</td>
    </tr>
    <tr>
      <td>Llama 3.1 Swallow 8B Instruct v0.2</td>
      <td>0.5584</td>
      <td>November 11, 2024</td>
    </tr>
    <tr>
      <td>Llama 3.1 Swallow 8B Instruct v0.3</td>
      <td>0.6424</td>
      <td>December 23, 2024</td>
    </tr>
  </tbody>
</table>

<p>Given these advancements, Llama 3.1 Swallow 8B Instruct v0.3 represents significant evolution as an instruction-tuned model, and we highly recommend utilizing this version for future applications.</p>

<h2 id="70b-base">70B Base</h2>

<p>Next, we analyze the performance of the Llama 3.1 Swallow 70B base model. The graph below shows the major base models with 20B parameters or more, ranked by their average scores on Japanese understanding and generation tasks.</p>

<p style="text-align: center;">
  <img src="images/20b-base-en.svg" alt="Average benchmark scores for major base models with over 20B parameters" />
</p>

<p>The average score for Japanese understanding and generation tasks with Llama 3.1 Swallow 70B is 0.5932, falling short of Qwen-2.5 72B’s score of 0.6232. Additionally, this score is nearly identical to the previous version, Llama 3 Swallow 70B, which scored 0.5934, suggesting that performance has not improved as much as we anticipated. One factor contributing to this outcome may be the emphasis on maintaining English capabilities during the continual pre-training.</p>

<p>The table below summarizes the average scores on Japanese and English understanding and generation tasks for Llama 3 and Llama 3 Swallow, as well as Llama 3.1 and Llama 3.1 Swallow, before and after continual pre-training. While the Llama 3 Swallow saw a decrease of 1.56 points in English understanding and generation scores, Llama 3.1 Swallow 70B experienced an increase of 1.46 points. This suggests that during the continual pre-training for Llama 3.1 Swallow 70B, the effort to maintain English performance may have led to a plateau in Japanese performance.</p>

<p>When constructing Llama 3.1 Swallow, we conducted validation experiments with the 8B model to find an optimal balance between Japanese and English performance and used this data to determine the composition of training data. However, as English performance actually increased in the 70B model, it may be necessary to adjust the corpus composition according to model scale (larger models like the 70B may be less prone to forgetting English). In any case, these results, which show an improvement in English performance, provide intriguing insights into how to build robust LLMs strong in both Japanese and English.</p>

<table>
  <thead>
    <tr>
      <th>Evaluation Task</th>
      <th style="text-align: right">Llama 3 70B</th>
      <th style="text-align: right">Llama 3 Swallow 70B</th>
      <th style="text-align: right">Llama 3.1 70B</th>
      <th style="text-align: right">Llama 3.1 Swallow 70B</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Japanese understanding &amp; Generation</td>
      <td style="text-align: right">0.5682</td>
      <td style="text-align: right">0.5934 (+0.0252)</td>
      <td style="text-align: right">0.5662</td>
      <td style="text-align: right">0.5932 (+0.0270)</td>
    </tr>
    <tr>
      <td>English understanding &amp; Generation</td>
      <td style="text-align: right">0.6905</td>
      <td style="text-align: right">0.6749 (-0.0156)</td>
      <td style="text-align: right">0.6748</td>
      <td style="text-align: right">0.6894 (+0.0146)</td>
    </tr>
  </tbody>
</table>

<p>Aside from Swallow, the standout models are Qwen2.5 72B and Gemma 2 27B. Despite being a multilingual model, Qwen2.5 72B achieved the highest score in this class with 0.6232. Although not shown in the graph above, the previous version, Qwen2 72B, scored 0.5937, showing steady improvement in version 2.5. Gemma 2 27B also performed remarkably well, surpassing larger models like the 35B and 47B Mixture-of-Expert (MoE) models and standing on par with models in the 70B class despite its smaller parameter number.</p>

<h2 id="70b-instruct">70B Instruct</h2>

<p>Finally, we analyze the performance of the Llama 3.1 Swallow 70B Instruct. The graph below shows the average scores on Japanese understanding and generation tasks, Japanese MT-bench, and English understanding and generation tasks for major base models with 20B parameters or more. In this graph, the models are arranged by average scores on Japanese MT-bench tasks.</p>

<p style="text-align: center;">
  <img src="images/20b-inst-en.svg" alt="Average benchmark scores for major instruction-tuned models with over 20B parameters" />
</p>

<p>The Japanese MT-bench average score for Llama 3.1 Swallow 70B Instruct v0.3 was 0.7115, surpassing GPT-3.5 (gpt-3.5-turbo-0125), which scored 0.6661.
Among the models evaluated this time, it ranked fourth, following GPT-4o (gpt-4o-2024-05-13) with 0.7791, Qwen2.5-72B-Instruct with 0.7594, and Llama 3 Youko 70B Instruct with 0.7222.
Additionally, it showed a 5.68-point improvement compared to the previous version, Llama 3.1 Swallow 70B Instruct v0.1, which scored 0.6547.
With the significant performance improvements of Japanese open LLMs, it may be time to incorporate human evaluations (Chatbot Arena: Chiang et al., 2024) or increase question difficulty (Arena-Hard: Li et al., 2024).</p>


          </div>
        </div>
      </div>
    </div>
</section>
<!-- end section -->

<!-- begin section -->
<section class="section projects" id="projects">
    <div class="container">
      <div class="row">
        <div class="col col-12">
          <div class="contaniner__inner animate">
            <div class="section__info">
              <div class="section__head">
                
                <a name="method">
                
                <h2 class="section__title"><span class="text-span">Method</span></h2>
                
                </a>
                
              </div>
            </div>
          </div>
          <div>
            
<p>Llama 3.1 Swallow is constructed following these steps:</p>

<ol>
  <li><strong>Llama 3.1 Swallow base model:</strong> Continual pre-training (Fujii et al., 2024) is conducted on the Llama 3.1 8B and 70B base models (without vocabulary expansion).</li>
  <li><strong>Llama 3.1 Swallow instruction-tuned Model:</strong> Supervised fine-tuning (SFT) is applied to the Llama 3.1 Swallow base model.</li>
</ol>

<h3 id="swallow-corpus-version-2">Swallow Corpus Version 2</h3>

<p>For the continual pre-training of Llama 3.1 Swallow, we created a Japanese web corpus (Swallow Corpus Version 2) by extracting and refining Japanese text from the entire archives of <a href="https://commoncrawl.org/">Common Crawl</a> (94 snapshots collected between 2013 and 2023, roughly 254.7 billion pages). Swallow Corpus Version 2 involved downloading these 254.7 billion pages, extracting around 8.3 billion pages identified to be in Japanese (roughly 12 trillion Japanese characters). The proportion of Japanese web pages in the Common Crawl was about 3.2%. This dataset is approximately four times the size of Swallow Corpus Version 1 (Okazaki et al., 2024) used for training Llama 3 Swallow (as measured by the total number of web pages used in corpus construction).</p>

<p>In Swallow Corpus Version 2, the corpus construction process was modified to facilitate the selection of text data suitable for LLM training. Common Crawl often revisits the same website at different times and may include similar content across different sites due to minor edits or reposting. To prevent models from memorizing data, it is essential to avoid repeatedly training on the same text. Therefore, a deduplication process is applied to the pre-training corpus, removing redundant text. This involves identifying sets of similar pages among billions, requiring substantial processing time and memory. Deduplication is thus a major challenge in corpus construction for pre-training.</p>

<p>As the number of web pages increases, so do the time and memory needed for deduplication. Thus, it is common to first filter (remove low-quality pages and select high-quality pages) to reduce the number of pages before deduplication. However, this process requires deduplication to be redone each time the filtering criteria are adjusted. To allow flexibility in experimenting with filtering methods, Swallow Corpus Version 2 performs deduplication first. Although this increases the necessary time and memory for deduplication, the Swallow team completed deduplication across all Japanese pages over about a month. The resulting Japanese web corpus after deduplication comprises 1.9 billion pages (3.2 trillion characters).</p>

<h3 id="selection-of-educationally-valuable-texts">Selection of Educationally Valuable Texts</h3>

<p>In the previous version, Llama 3 Swallow, efforts were made to improve Japanese knowledge-related tasks such as question answering (NIILC) and machine translation (WMT20) by carefully blending the pre-training datasets. However, no significant improvement was observed on general knowledge tasks (JMMLU), which became a challenge for the Swallow project. Thus, inspired by recent research (FineWeb-Edu <a href="https://arxiv.org/abs/2406.17557">(Penedo et al., 2024)</a>, DataComp-LM <a href="https://arxiv.org/abs/2406.11794">(Li et al., 2024)</a>, Dolma 1.7 <a href="https://blog.allenai.org/olmo-1-7-7b-a-24-point-improvement-on-mmlu-92b43f7d269d">(Ai2, 2024)</a>), Llama 3.1 Swallow adopted an approach of carefully selecting small amounts of “educational” text to enhance performance on general knowledge tasks.</p>

<p>Specifically, Japanese Wikipedia articles in academic fields like STEM and social sciences were considered examples of “educational” text. About 60,000 such documents were selected as positive examples to train a FastText classifier for educational text. This classifier achieved over 99% accuracy on the validation dataset, effectively identifying documents similar to academic Wikipedia articles with high precision. Additionally, since FastText classifiers run efficiently on CPUs, it is a lightweight classifier that scales to the size of Swallow Corpus Version 2.</p>

<p>With this classifier, quality filtering could be performed independently of the heuristic rules (such as the proportion of Hiragana characters) used in Swallow Corpus Version 1. The team verified the effectiveness of these heuristics, retaining only the appropriate ones and combining them with the classifier for quality filtering. This approach not only removed low-quality documents but also aimed to extract more “educational” documents.</p>

<p><strong>Examples of Discontinual Rules</strong></p>

<ul>
  <li>Length of the longest sentence in the text (texts were removed if they contained sentences that were too long)</li>
  <li>Proportion of repeated n-grams (texts were removed if the proportion was high)</li>
</ul>

<p><strong>Examples of Rules Retained</strong></p>

<ul>
  <li>Average length of sentences in the text (texts were removed if the length was too high or too low)</li>
  <li>Proportion of Hiragana in the text (texts were removed if the proportion was too low)</li>
</ul>

<p>To evaluate these efforts, ablation experiments were conducted before constructing Llama 3.1 Swallow. The results showed that the introduction of the classifier and adjustments to the heuristic rules improved performance on multiple Japanese tasks, including general knowledge (JMMLU) and translation (WMT20). Notably, combining heuristics with the classifier achieved the highest performance, demonstrating the effectiveness of extracting “educational” Japanese text.</p>

<p>However, arithmetic reasoning (MGSM) showed a negative impact. This issue was mitigated by adding a specific mathematical dataset (Cosmopedia) during the training of Llama 3.1 Swallow, so it is not expected to be a problem in the actual continual pre-training process.</p>

<table>
  <thead>
    <tr>
      <th>Experimental Pattern</th>
      <th>JCom.</th>
      <th>JEMHopQA</th>
      <th>NIILC</th>
      <th>JSQuAD</th>
      <th>XLSum</th>
      <th>MGSM</th>
      <th>WMT-20 (en-ja)</th>
      <th>WMT-20 (ja-en)</th>
      <th>JMMLU</th>
      <th>JHumanEval</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Traditional Heuristics</td>
      <td>88.6</td>
      <td>45.6</td>
      <td>56.1</td>
      <td>89.1</td>
      <td>19.7</td>
      <td><strong>34.4</strong></td>
      <td>26.1</td>
      <td>17.8</td>
      <td>45.7</td>
      <td>22.3</td>
    </tr>
    <tr>
      <td>Combination of Traditional Heuristics and Classifier</td>
      <td>88.5</td>
      <td>53.4</td>
      <td>59.5</td>
      <td>89.2</td>
      <td>19.7</td>
      <td>27.2</td>
      <td>28.0</td>
      <td>18.5</td>
      <td>46.4</td>
      <td>23.1</td>
    </tr>
    <tr>
      <td>Combination of Adjusted Heuristics and Classifier</td>
      <td><strong>89.1</strong></td>
      <td><strong>55.3</strong></td>
      <td><strong>60.7</strong></td>
      <td><strong>89.5</strong></td>
      <td><strong>20.9</strong></td>
      <td>28.4</td>
      <td><strong>29.7</strong></td>
      <td><strong>22.6</strong></td>
      <td><strong>48.4</strong></td>
      <td><strong>24.1</strong></td>
    </tr>
  </tbody>
</table>

<p><strong>Ablation Experiment Settings</strong></p>
<ul>
  <li>Continual pre-training on 50B tokens of Japanese corpus based on Llama 3 8B</li>
  <li>The Japanese corpus consisted of Japanese Wikipedia (1.69B tokens) and Swallow Corpus (48.31B tokens), with only the quality filtering settings for the Swallow Corpus modified</li>
  <li>In experiments using the classifier, only the top 10% of texts by classifier score were extracted from the Swallow Corpus after applying heuristic rules</li>
</ul>

<p>During the construction of Llama 3.1 Swallow, improvements exceeding 8% on general knowledge (JMMLU) for the 8B model compared to Llama 3 Swallow confirmed the effectiveness of selecting educationally valuable texts.</p>

<h2 id="maintaining-english-proficiency-during-continual-pre-training">Maintaining English Proficiency During Continual Pre-training</h2>

<p>When enhancing the Japanese capabilities of an LLM through continual pre-training, a decline in the model’s original capabilities — primarily its English understanding and generation abilities — is often observed. For example, continual pre-training from Llama 2 7B to Swallow 7B resulted in a 6.1-point drop in the average score for English understanding and generation tasks, while the continual pre-training from Llama 2 70B to Swallow 70B saw a 2.7-point decrease. While it is essential to anticipate some loss in English proficiency when teaching Japanese to an LLM, maintaining these original abilities is desirable for tasks like arithmetic reasoning, general knowledge, and code generation, as these skills transfer well from English to Japanese.</p>

<p>In developing Llama 3.1 Swallow, we carefully selected training data to improve performance on general knowledge and code generation tasks. We decided to use datasets like DataComp-baseline, which showed effectiveness in general knowledge tasks, and The Stack v2, which demonstrated positive results in code generation tasks. Preliminary experiments also explored the optimal blend of these datasets. As a result, continual pre-training from Llama 3.1 8B to Llama 3.1 Swallow 8B led to only a 0.6-point decrease in the average English understanding and generation score, while training from Llama 3.1 70B to Llama 3.1 Swallow 70B even resulted in a 1.4-point improvement.</p>

<p>The radar chart below illustrates the scores on English understanding and generation tasks before and after continual pre-training for Swallow 7B and Llama 3.1 Swallow 8B. While Swallow 7B shows noticeable declines across tasks, Llama 3.1 Swallow 8B demonstrates minimal score reduction. Insights into dataset selection and composition like this are vital for exploring methods to build LLMs strong in both Japanese and English.</p>

<p style="text-align: center;">
  <img src="images/rader-en.svg" alt="Changes in English understanding and generation task scores before and after continual pre-training" />
</p>

<p>The corpora used for continual pre-training are as follows:</p>

<ul>
  <li><a href="https://huggingface.co/datasets/HuggingFaceTB/cosmopedia">Cosmopedia</a></li>
  <li><a href="https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0">Dclm-baseline-1.0</a> (Li et al., 2024)</li>
  <li><a href="https://dumps.wikimedia.org/other/cirrussearch">English Wikipedia</a></li>
  <li><a href="https://dumps.wikimedia.org/other/cirrussearch">Japanese Wikipedia</a></li>
  <li><a href="https://github.com/laboroai/Laboro-ParaCorpus">Laboro ParaCorpus</a></li>
  <li><a href="https://arxiv.org/abs/2404.17733">Swallow Corpus Version 2</a></li>
  <li><a href="https://huggingface.co/datasets/bigcode/the-stack-v2-train-smol-ids">The Stack v2</a> (Lozhkov et al., 2024)</li>
</ul>

<h2 id="enhancing-conversational-ability-with-synthetic-data">Enhancing Conversational Ability with Synthetic Data</h2>

<p>Improving LLM’s conversational ability hinges on instruction tuning with diverse and complex prompts, as well as useful and fluent responses. Ideally, it would involve collecting real-world user prompts and manually providing suitable responses, but this process is extremely time-consuming and labor-intensive. To build training data more quickly and affordably, the research team opted to imitate responses from existing high-performance LLMs. Specifically, prompts from the LMSYS-Chat-1M dataset, which contains human-computer interaction data, were translated into Japanese. Then, responses were automatically generated using an open LLM with top-tier conversational ability. Following the methodology used in Llama 3.1 construction, multiple responses were generated, and the best response was selected through automatic scoring by an LLM. Additionally, quality was improved by identifying and removing duplicate prompts, template-based prompts, and unnecessary repetitive responses.</p>

<p>The datasets used for instruction tuning are as follows:</p>

<h3 id="instruction-tuning-data-for-llama-31-swallow-instruct-v03">Instruction Tuning Data for Llama 3.1 Swallow Instruct v0.3</h3>

<p>Japanese instruction tuning data covers following datasets.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">lmsys-chat-1m-synth-ja-gemma2-2turns-wo-pii-and-template-instructions</code>: A Japanese multi-turn instruction-response dataset synthesized from <a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m">lmsys-chat-1m</a> (Zhang et al., 2024). The first-turn human prompts were translated into Japanese using DeepL (machine translation), and responses to the translated prompts were generated with <a href="https://huggingface.co/google/gemma-2-27b-it">gemma-2-27b-it</a>. Afterward, rejection sampling (n=6) was performed using <a href="https://huggingface.co/google/gemma-2-27b-it">gemma-2-27b-it</a> for automated scoring. Second-turn user instructions and responses were synthesized using <a href="https://huggingface.co/google/gemma-2-27b-it">gemma-2-27b-it</a>. The quality of the second-turn responses was scored by <a href="https://huggingface.co/google/gemma-2-27b-it">gemma-2-27b-it</a> on a scale of of 1-10. Second-turn responses with scores lower than 9 were rejected, along with their corresponding instructions. Dialogues containing personally identifiable information (PII), template-based prompts, and duplicate prompts were removed.</li>
  <li><code class="language-plaintext highlighter-rouge">filtered-magpie-ultra-ja</code>: The Japanese version of the <code class="language-plaintext highlighter-rouge">filtered-magpie-ultra-en</code> dataset, translated into Japanese using <a href="https://huggingface.co/google/gemma-2-27b-it">gemma-2-27b-it</a>.</li>
  <li><code class="language-plaintext highlighter-rouge">gemma-magpie-greater-than-7</code>: A Japanese question-answering dataset synthesized from scratch using <a href="https://huggingface.co/google/gemma-2-27b-it">gemma-2-27b-it</a>. Prompts were generated with topic-specific prompts, and assistant responses were generated for these prompts. Filtering was then applied based on heuristic rules for quality and length. Then, <a href="https://huggingface.co/google/gemma-2-27b-it">gemma-2-27b-it</a> was applied to score the quality of each of the reponse with a range of 1-10. Responses with scores lower or equal to 7, along with its corresponding instructions, were rejected.</li>
</ul>

<p>English data are excluded during the instruction tuning of Llama-3.1-Swallow-Instruct-v0.3.</p>

<h3 id="instruction-tuning-data-for-llama-31-swallow-instruct-v01-and-v02">Instruction Tuning Data for Llama 3.1 Swallow Instruct v0.1 and v0.2</h3>

<p>Japanese instruction tuning data covers following datasets.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">lmsys-chat-1m-synth-ja-wo-pii-and-template-instructions</code>: A Japanese single-turn instruction-response dataset synthesized from <a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m">lmsys-chat-1m</a> (Zhang et al., 2024). The first-turn human prompts were translated into Japanese using DeepL (machine translation), and responses to the translated prompts were generated with <a href="https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct">Llama-3.1-405B-Instruct</a>. Afterward, rejection sampling (n=6) was performed using <a href="https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct">Llama-3.1-70B-Instruct</a> for automated scoring. Dialogues containing personally identifiable information (PII), template-based prompts, and duplicate prompts were removed.</li>
  <li><code class="language-plaintext highlighter-rouge">filtered-magpie-ultra-ja</code>: The Japanese version of the <code class="language-plaintext highlighter-rouge">filtered-magpie-ultra-en</code> dataset, translated into Japanese using <a href="https://huggingface.co/google/gemma-2-27b-it">gemma-2-27b-it</a>.</li>
  <li><code class="language-plaintext highlighter-rouge">gemma-magpie</code>: A Japanese question-answer dataset synthesized from scratch using <a href="https://huggingface.co/google/gemma-2-27b-it">gemma-2-27b-it</a>. Prompts were generated with topic-specific prompts, and assistant responses were generated for these prompts. Filtering was then applied based on heuristic rules for quality and length.</li>
</ul>

<p>English instruction tuning data covers following datasets.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">lmsys-chat-1m-synth-en-wo-pii-and-template-instructions</code>: Responses to the original English prompts from <a href="https://huggingface.co/datasets/lmsys/lmsys-chat-1m">lmsys-chat-1m</a> were generated following the same method used for the Japanese dataset, <code class="language-plaintext highlighter-rouge">lmsys-chat-1m-synth-ja-wo-pii-and-template-instructions</code>. Unlike the Japanese version, rejection sampling was omitted.</li>
  <li><code class="language-plaintext highlighter-rouge">filtered-magpie-ultra-en</code>: A subset of the MAGPIE instruction tuning data, created by <a href="https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct">Llama-3.1-405B-Instruct</a>, following the MAGPIE methodology (Xu et al., 2024) <a href="https://huggingface.co/datasets/argilla/magpie-ultra-v0.1">magpie-ultra</a>. Specifically, only examples rated above average were selected.</li>
</ul>

<h2 id="improving-processing-speed-in-distributed-parallel-training">Improving Processing Speed in Distributed Parallel Training</h2>

<p>In training LLMs, distributed parallel training using multiple GPUs is essential. Increasing the number of GPUs improves overall processing speed, but communication between GPUs can become a bottleneck, leading to decreased processing speed (computational efficiency) per GPU. To mitigate this, we introduced a method to finely interweave computation and communication, helping to maintain computational efficiency. Additionally, we revisited the distributed parallel training configurations to find the optimal settings for training Llama 3.1 Swallow. Below shows the processing speed per GPU (TFLOP/s), or computational efficiency, during continual pre-training for Llama 3.1 Swallow. As shown in the graph, with a micro-batch size of 2, we confirmed that even when training the 8B model of Llama 3.1 Swallow on 128 GPUs (16 nodes) using A100 (40GB), computational efficiency (184.9 TFLOP/s) equal to or exceeding that of training on 8 GPUs (1 node) can be achieved.</p>

<p style="text-align: center;">
  <img src="images/flops-en.svg" alt="Computational efficiency per GPU in continual pre-training of Llama 3.1 Swallow" />
</p>

<p>Furthermore, increasing the number of GPUs occasionally led to unintended training interruptions, which negatively affected the training efficiency of LLMs. In the continual pre-training of Llama 3.1 Swallow, adjustments to communication settings significantly reduced such interruptions, enhancing the utilization efficiency of computational resources.</p>

<h3 id="references">References</h3>

<ul>
  <li>Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, Hao Zhang, Michael I. Jordan, Joseph E. Gonzalez and Ion Stoica. 2024. <a href="https://openreview.net/forum?id=3MW8GKNyzI">Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference</a>. The Forty-first International Conference on Machine Learning (ICML), July 2024.</li>
  <li>Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Hiroki Iida, Masanari Ohi, Kakeru Hattori, Hirai Shota, Sakae Mizuki, Rio Yokota, and Naoaki Okazaki. <a href="https://openreview.net/forum?id=TQdd1VhWbe">Continual Pre-Training for Cross-Lingual LLM Adaptation: Enhancing Japanese Language Capabilities</a>. In Proceedings of the First Conference on Language Modeling (COLM), October 2024.</li>
  <li>Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Kumar Guha, Sedrick Keh, Kushal Arora, Saurabh Garg, Rui Xin, Niklas Muennighoff, Reinhard Heckel, Jean Mercat, Mayee Chen, Suchin Gururangan, Mitchell Wortsman, Alon Albalak, Yonatan Bitton, Marianna Nezhurina, Amro Abbas, Cheng-Yu Hsieh, Dhruba Ghosh, Josh Gardner, Maciej Kilian, Hanlin Zhang, Rulin Shao, Sarah M. Pratt, Sunny Sanyal, Gabriel Ilharco, Giannis Daras, Kalyani Marathe, Aaron Gokaslan, Jieyu Zhang, Khyathi Raghavi Chandu, Thao Nguyen, Igor Vasiljevic, Sham M. Kakade, Shuran Song, Sujay Sanghavi, Fartash Faghri, Sewoong Oh, Luke Zettlemoyer, Kyle Lo, Alaaeldin El-Nouby, Hadi Pouransari, Alexander Toshev, Stephanie Wang, Dirk Groeneveld, Luca Soldaini, Pang Wei Koh, Jenia Jitsev, Thomas Kollar, Alexandros G. Dimakis, Yair Carmon, Achal Dave, Ludwig Schmidt and Vaishaal Shankar. 2024. <a href="https://arxiv.org/abs/2406.11794">DataComp-LM: In search of the next generation of training sets for language models</a>. arXiv:2406.11794.</li>
  <li>Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez and Ion Stoica. 2024. <a href="https://arxiv.org/abs/2406.11939">From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline</a>. arXiv:2406.11939.</li>
  <li>Naoaki Okazaki, Kakeru Hattori, Hirai Shota, Hiroki Iida, Masanari Ohi, Kazuki Fujii, Taishi Nakamura, Mengsay Loem, Rio Yokota, and Sakae Mizuki. <a href="https://openreview.net/forum?id=N5EYQSwW26">Building a Large Japanese Web Corpus for Large Language Models</a>. In Proceedings of the First Conference on Language Modeling (COLM), October 2024.</li>
  <li>Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, Bill Yuchen Lin. 2024. <a href="https://arxiv.org/abs/2406.08464">Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing</a>. arXiv:2406.08464.</li>
  <li>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric Xing, Joseph E. Gonzalez, Ion Stoica and Hao Zhang. 2024. <a href="https://openreview.net/forum?id=BOfDKxfwt0">LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset</a>. The Twelfth International Conference on Learning Representations (ICLR), May 2024.</li>
</ul>


          </div>
        </div>
      </div>
    </div>
</section>
<!-- end section -->

<!-- begin section -->
<section class="section projects" id="projects">
    <div class="container">
      <div class="row">
        <div class="col col-12">
          <div class="contaniner__inner animate">
            <div class="section__info">
              <div class="section__head">
                
                <a name="acknowledgement">
                
                <h2 class="section__title"><span class="text-span">Appendix</span></h2>
                
                </a>
                
              </div>
            </div>
          </div>
          <div>
            
<p>The research and development of the large language model Swallow has been supported by the AIST Project “Research and Development on Generative AI Foundation Models in the Physical Domain,” the “Core Integrated Technology Development for Next-Generation Artificial Intelligence and Robotics” project by the New Energy and Industrial Technology Development Organization (NEDO) (JPNP18002), specifically focusing on “Development of AI Application Technology for Decision Support in Design Risk Assessment Based on Expert Perspectives.” It is also supported by a project from the Ministry of Education, Culture, Sports, Science, and Technology (MEXT) aimed at “establishment of research and development centers to ensure the transparency and reliability of generative AI models”, along with other contributions. Additionally, data and insights developed by LLM-jp (LLM Study Group) were utilized for evaluating LLMs.</p>


          </div>
        </div>
      </div>
    </div>
</section>
<!-- end section -->


  </main>
  <!-- end content -->

  <!-- begin contact -->
  <div class="contact-modal">
    <div class="container">
      <div class="row">
        <div class="col col-12">

          <div class="contact-close">
            <i class="ion ion-md-close"></i>
          </div>

          <div class="form-box">
            <div class="contact-head">
              
                <h2 class="contact-title"><span class="text-span">Get</span> in touch</h2>
              
              
                <p class="contact-description">Avenco comes with a built-in contact form.</p>
              
            </div>
            <form class="form" action="https://formspree.io/" method="POST">
              <div class="form__group">
                <label class="form__label screen-reader-text" for="form-name">Your Name</label>
                <input class="form__input" id="form-name" type="text" name="name" placeholder="Your name..." required>
              </div>
              <div class="form__group">
                <label class="form__label screen-reader-text" for="form-email">Your Email</label>
                <input class="form__input" id="form-email" type="email" name="_replyto" placeholder="Your email..." required>
              </div>
              <div class="form__group">
                <label class="form__label screen-reader-text" for="form-text">Your Message</label>
                <textarea class="form__input" id="form-text" name="text" rows="8" placeholder="Your message..." required></textarea>
              </div>
              <div class="form__group form__group-button">
                <button class="button button--middle" type="submit">Send now <i class="ion ion-ios-paper-plane"></i></button>
              </div>
            </form>
          </div>

        </div>
      </div>
    </div>
  </div>
<!-- end contact -->

  <div class="top" title="Top"><i class="ion ion-ios-arrow-up"></i></div>

  <!-- begin footer -->
<footer class="footer">

  
    <div class="footer-cover">
      <div class="cover-image">
        <img class="lazy" data-src="images/llama3.1-swallow.png" alt="Swallow LLM">
      </div>
    </div>
  

  <div class="container">
    <div class="row">
      <div class="col col-12">
        
  <div class="social">
    <ul class="social__list list-reset">
      
      <li class="social__item">
        <a class="social__link" href="https://github.com/tokyotech-llm/" target="_blank" rel="noopener" aria-label="social icon"><i class="ion ion-logo-github"></i></a>
      </li>
      
    </ul>
  </div>

        <div class="copyright">2024 &copy; <a href="/">Swallow LLM</a>. Crafted & Designed by <a href="https://jekyllthemes.io/developers/artem-sheludko">Artem Sheludko</a>.</div>
      </div>
    </div>
  </div>
</footer>
<!-- end footer -->

  <script src='https://swallow-llm.github.io/js/scripts.js'></script>
  <script>
    var itype_text = [ 'Get inspiration, updates and', 'cool stuff about design!' ];
  </script>
  <script src='https://swallow-llm.github.io/js/common.js'></script>
</body>

</html>